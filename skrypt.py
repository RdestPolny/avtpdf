"""
Redaktor AI - Interaktywny Procesor Dokument√≥w z PaddleOCR
===========================================================

INSTALACJA LOKALNA:
-------------------
# 1. Zale≈ºno≈õci systemowe (dla Linux/Docker/WSL)
# sudo apt-get update && sudo apt-get install -y libgl1-mesa-glx libglib2.0-0

# 2. Zale≈ºno≈õci Python
pip install -r requirements.txt

WDRO≈ªENIE NA STREAMLIT CLOUD:
-----------------------------
W repozytorium muszƒÖ znajdowaƒá siƒô pliki:
1. `requirements.txt` (z listƒÖ bibliotek Python)
2. `packages.txt` (z listƒÖ bibliotek systemowych, np. libgl1-mesa-glx)

URUCHOMIENIE LOKALNE:
--------------------
streamlit run redaktor_ai_enhanced.py
"""

import streamlit as st
import fitz  # PyMuPDF
from openai import AsyncOpenAI
import io
import zipfile
import json
from pathlib import Path
import re
import asyncio
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

# Opcjonalne importy
try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import mammoth
    MAMMOTH_AVAILABLE = True
except ImportError:
    MAMMOTH_AVAILABLE = False

# Importy PaddleOCR
PADDLEOCR_AVAILABLE = False
try:
    import numpy as np
    from PIL import Image
    # Sprawdzamy, czy mo≈ºna zaimportowaƒá g≈Ç√≥wnƒÖ klasƒô
    from paddleocr import PaddleOCR
    PADDLEOCR_AVAILABLE = True
except ImportError:
    pass

# ===== KONFIGURACJA =====

PROJECTS_DIR = Path("pdf_processor_projects")
BATCH_SIZE = 10
MAX_RETRIES = 3
DEFAULT_MODEL = 'gpt-4o-mini'

# Konfiguracja OCR
OCR_CONFIDENCE_THRESHOLD = 0.6
NATIVE_TEXT_MIN_LENGTH = 50

SESSION_STATE_DEFAULTS = {
    'processing_status': 'idle',
    'document': None,
    'current_page': 0,
    'total_pages': 0,
    'extracted_pages': [],
    'project_name': None,
    'next_batch_start_index': 0,
    'uploaded_filename': None,
    'api_key': None,
    'model': DEFAULT_MODEL,
    'meta_tags': {},
    'project_loaded_and_waiting_for_file': False,
    'processing_mode': 'all',
    'start_page': 1,
    'end_page': 1,
    'processing_end_page_index': 0,
    'article_page_groups_input': '',
    'article_groups': [],
    'next_article_index': 0,
    'file_type': None,
    'ocr_mode': 'paddleocr',
    'ocr_language': 'pl',
    'optimized_articles': {}
}

# ===== KLASY POMOCNICZE =====

@dataclass
class PageContent:
    """Reprezentuje zawarto≈õƒá pojedynczej strony"""
    page_number: int
    text: str
    images: List[Dict] = None
    extraction_method: str = "native"
    ocr_confidence: float = 0.0
    
    def __post_init__(self):
        if self.images is None:
            self.images = []

def get_ocr_engine(language: str = 'pl'):
    """
    Pobiera lub tworzy instancjƒô silnika PaddleOCR, bezpiecznie przechowujƒÖc jƒÖ w st.session_state.
    Jest to zalecany wzorzec singleton dla aplikacji Streamlit.
    """
    session_key = f"paddleocr_engine_{language}"

    if session_key in st.session_state and st.session_state[session_key] is not None:
        return st.session_state[session_key]

    if not PADDLEOCR_AVAILABLE:
        st.error("Biblioteka PaddleOCR nie jest zainstalowana! Uruchom: pip install paddleocr")
        return None

    try:
        from paddleocr import PaddleOCR
        
        with st.spinner(f"Inicjalizacja silnika OCR dla jƒôzyka '{language}'..."):
            # === POPRAWKA TUTAJ ===
            # Usuniƒôto przestarza≈Çy argument `show_log=False`, kt√≥ry powodowa≈Ç b≈ÇƒÖd.
            ocr_engine = PaddleOCR(
                use_angle_cls=True,
                lang=language,
                use_gpu=False
            )
        
        st.session_state[session_key] = ocr_engine
        st.toast(f"‚úÖ Silnik OCR gotowy!", icon="üöÄ")
        
        return ocr_engine

    except Exception as e:
        st.error(f"Krytyczny b≈ÇƒÖd inicjalizacji PaddleOCR: {e}")
        st.info("üí° Je≈õli u≈ºywasz Streamlit Cloud, upewnij siƒô, ≈ºe plik 'packages.txt' z 'libgl1-mesa-glx' jest w repozytorium. "
                "W innym przypadku, zainstaluj brakujƒÖce biblioteki systemowe.")
        return None

def extract_text_with_paddleocr(image_data: bytes, language: str = 'pl') -> Tuple[str, float]:
    """
    WyciƒÖga tekst z obrazu u≈ºywajƒÖc PaddleOCR.
    Zwraca: (text, average_confidence)
    """
    try:
        ocr = get_ocr_engine(language)
        
        if ocr is None:
            st.warning("Silnik OCR nie jest dostƒôpny z powodu b≈Çƒôdu. Ekstrakcja pominiƒôta.")
            return "", 0.0

        img = Image.open(io.BytesIO(image_data))
        img_array = np.array(img)
        
        result = ocr.ocr(img_array, cls=True)
        
        if not result or not result[0]:
            return "", 0.0
        
        texts = []
        confidences = []
        
        for line in result[0]:
            if line:
                text = line[1][0]
                confidence = line[1][1]
                texts.append(text)
                confidences.append(confidence)
        
        full_text = "\n".join(texts)
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        return full_text, avg_confidence
        
    except Exception as e:
        st.warning(f"B≈ÇƒÖd podczas przetwarzania obrazu przez PaddleOCR: {e}")
        return "", 0.0

class DocumentHandler:
    """Klasa do obs≈Çugi r√≥≈ºnych format√≥w dokument√≥w - PaddleOCR jako g≈Ç√≥wny silnik"""
    
    def __init__(self, file_bytes: bytes, filename: str):
        self.file_bytes = file_bytes
        self.filename = filename
        self.file_type = self._detect_file_type(filename)
        self._document = None
        self._html_content = None
        self._load_document()
    
    def _detect_file_type(self, filename: str) -> str:
        ext = Path(filename).suffix.lower()
        if ext == '.pdf':
            return 'pdf'
        elif ext == '.docx':
            if not DOCX_AVAILABLE:
                raise ValueError("Format DOCX nie jest obs≈Çugiwany. Zainstaluj: pip install python-docx")
            return 'docx'
        elif ext == '.doc':
            if not MAMMOTH_AVAILABLE:
                raise ValueError("Format DOC nie jest obs≈Çugiwany. Zainstaluj: pip install mammoth")
            return 'doc'
        else:
            raise ValueError(f"Nieobs≈Çugiwany format pliku: {ext}")
    
    def _load_document(self):
        """≈Åaduje dokument w odpowiednim formacie"""
        if self.file_type == 'pdf':
            self._document = fitz.open(stream=self.file_bytes, filetype="pdf")
        elif self.file_type == 'docx':
            self._document = DocxDocument(io.BytesIO(self.file_bytes))
        elif self.file_type == 'doc':
            result = mammoth.convert_to_html(io.BytesIO(self.file_bytes))
            self._html_content = result.value
            self._document = None
    
    def get_page_count(self) -> int:
        """Zwraca liczbƒô stron w dokumencie"""
        if self.file_type == 'pdf':
            return len(self._document)
        elif self.file_type == 'docx':
            all_text = '\n\n'.join([p.text for p in self._document.paragraphs])
            words = all_text.split()
            return max(1, len(words) // 500 + (1 if len(words) % 500 > 0 else 0))
        elif self.file_type == 'doc':
            words = self._html_content.split()
            return max(1, len(words) // 500 + (1 if len(words) % 500 > 0 else 0))
        return 0
    
    def _should_use_ocr_primary(self, page_index: int) -> bool:
        """
        Logika wyboru silnika ekstrakcji.
        """
        if not PADDLEOCR_AVAILABLE:
            return False
        
        ocr_mode = st.session_state.get('ocr_mode', 'paddleocr')
        
        if ocr_mode == 'native':
            return False
        
        if ocr_mode == 'paddleocr':
            return True
        
        # Tryb 'auto'
        if self.file_type != 'pdf':
            return False
        
        try:
            page = self._document.load_page(page_index)
            native_text = page.get_text("text")
            
            if len(native_text.strip()) < NATIVE_TEXT_MIN_LENGTH:
                return True
            
            text_blocks = page.get_text("blocks")
            if not text_blocks or len(text_blocks) == 0:
                return True
            
            return False
            
        except:
            return True
    
    def get_page_content(self, page_index: int, force_mode: str = None) -> PageContent:
        """
        Pobiera zawarto≈õƒá strony, u≈ºywajƒÖc PaddleOCR jako g≈Ç√≥wnego silnika.
        """
        if self.file_type != 'pdf':
            return self._get_non_pdf_content(page_index)
        
        page = self._document.load_page(page_index)
        images = self._extract_images_from_pdf_page(page_index)
        
        use_ocr = force_mode == 'paddleocr' if force_mode else self._should_use_ocr_primary(page_index)
        
        if not use_ocr or not PADDLEOCR_AVAILABLE:
            native_text = page.get_text("text")
            return PageContent(
                page_number=page_index + 1,
                text=native_text,
                images=images,
                extraction_method="native"
            )
        
        try:
            pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))
            img_bytes = pix.tobytes("png")
            
            language = st.session_state.get('ocr_language', 'pl')
            ocr_text, confidence = extract_text_with_paddleocr(img_bytes, language)
            
            if confidence > OCR_CONFIDENCE_THRESHOLD or len(ocr_text.strip()) > 50:
                return PageContent(
                    page_number=page_index + 1,
                    text=ocr_text,
                    images=images,
                    extraction_method="paddleocr",
                    ocr_confidence=confidence
                )
            else:
                native_text = page.get_text("text")
                if len(native_text.strip()) > len(ocr_text.strip()):
                    return PageContent(
                        page_number=page_index + 1,
                        text=native_text,
                        images=images,
                        extraction_method="hybrid_native",
                        ocr_confidence=confidence
                    )
                else:
                    return PageContent(
                        page_number=page_index + 1,
                        text=ocr_text,
                        images=images,
                        extraction_method="paddleocr_low_conf",
                        ocr_confidence=confidence
                    )
                
        except Exception as e:
            st.warning(f"OCR nie powiod≈Ço siƒô dla strony {page_index + 1}: {e}. U≈ºywam metody natywnej.")
            native_text = page.get_text("text")
            return PageContent(
                page_number=page_index + 1,
                text=native_text,
                images=images,
                extraction_method="native_fallback"
            )
    
    def _get_non_pdf_content(self, page_index: int) -> PageContent:
        if self.file_type == 'docx':
            return self._get_docx_page_content(page_index)
        elif self.file_type == 'doc':
            return self._get_doc_page_content(page_index)
    
    def _get_docx_page_content(self, page_index: int) -> PageContent:
        all_paragraphs = self._document.paragraphs
        words_per_page = 500
        
        all_text = '\n\n'.join([p.text for p in all_paragraphs])
        words = all_text.split()
        
        start_word = page_index * words_per_page
        end_word = min(start_word + words_per_page, len(words))
        
        page_text = ' '.join(words[start_word:end_word])
        images = self._extract_images_from_docx()
        
        return PageContent(page_index + 1, page_text, images)
    
    def _get_doc_page_content(self, page_index: int) -> PageContent:
        text = re.sub('<[^<]+?>', '', self._html_content)
        words = text.split()
        words_per_page = 500
        
        start_word = page_index * words_per_page
        end_word = min(start_word + words_per_page, len(words))
        
        page_text = ' '.join(words[start_word:end_word])
        
        return PageContent(page_index + 1, page_text, [])
    
    def _extract_images_from_pdf_page(self, page_index: int) -> List[Dict]:
        images = []
        if self.file_type != 'pdf':
            return images
        
        try:
            page = self._document.load_page(page_index)
            for img_index, img in enumerate(page.get_images(full=True)):
                xref = img[0]
                base_image = self._document.extract_image(xref)
                if base_image and base_image.get("width", 0) > 100 and base_image.get("height", 0) > 100:
                    images.append({
                        'image': base_image['image'],
                        'ext': base_image['ext'],
                        'index': img_index
                    })
        except Exception as e:
            st.warning(f"Nie uda≈Ço siƒô wyekstraktowaƒá obraz√≥w ze strony {page_index + 1}: {e}")
        
        return images
    
    def _extract_images_from_docx(self) -> List[Dict]:
        images = []
        try:
            for rel in self._document.part.rels.values():
                if "image" in rel.target_ref:
                    img_data = rel.target_part.blob
                    ext = rel.target_ref.split('.')[-1]
                    images.append({
                        'image': img_data,
                        'ext': ext,
                        'index': len(images)
                    })
        except Exception as e:
            st.warning(f"Nie uda≈Ço siƒô wyekstraktowaƒá obraz√≥w z DOCX: {e}")
        
        return images
    
    def render_page_as_image(self, page_index: int) -> Optional[bytes]:
        if self.file_type != 'pdf':
            return None
        
        try:
            page = self._document.load_page(page_index)
            pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))
            return pix.tobytes("png")
        except Exception as e:
            st.error(f"B≈ÇƒÖd podczas renderowania strony {page_index + 1}: {e}")
            return None

# ===== LOGIKA AI (bez zmian) =====
class AIProcessor:
    def __init__(self, api_key: str, model: str = DEFAULT_MODEL):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
    def get_system_prompt(self) -> str:
        return """Jeste≈õ precyzyjnym asystentem redakcyjnym. Twoim celem jest przekszta≈Çcenie surowego tekstu w czytelny, dobrze zorganizowany artyku≈Ç internetowy.
ZASADA NADRZƒòDNA: WIERNO≈öƒÜ TRE≈öCI, ELASTYCZNO≈öƒÜ FORMY.
- Nie zmieniaj oryginalnych sformu≈Çowa≈Ñ ani nie parafrazuj tekstu. Przenie≈õ tre≈õƒá 1:1.
- Twoja rola polega na dodawaniu element√≥w strukturalnych (nag≈Ç√≥wki, pogrubienia, podzia≈Ç na akapity).
INSTRUKCJE SPECJALNE:
1. Ignoruj i pomijaj numery stron oraz rozstrzelone daty.
2. Etykiety jak "NEWS FLASH" u≈ºywaj jako kontekstu, ale nie umieszczaj ich w finalnym tek≈õcie.
3. Je≈õli tekst zawiera b≈Çƒôdy OCR lub dziwne znaki, spr√≥buj je poprawiƒá w kontek≈õcie.
DOZWOLONE MODYFIKACJE STRUKTURALNE:
1. Tytu≈Ç G≈Ç√≥wny: `# Tytu≈Ç`
2. ≈ör√≥dtytu≈Çy: `## ≈ör√≥dtytu≈Ç` (u≈ºywaj ich do rozbijania '≈õciany tekstu').
3. Pogrubienia: `**tekst**` (dla kluczowych termin√≥w i nazw w≈Çasnych).
4. Podzia≈Ç na sekcje: `---` (je≈õli na stronie sƒÖ dwa niepowiƒÖzane tematy).
WYMAGANIA KRYTYCZNE:
- Twoja odpowied≈∫ musi byƒá WY≈ÅƒÑCZNIE i BEZWZGLƒòDNIE poprawnym obiektem JSON.
- NIE u≈ºywaj markdown code blocks (```json). Zwr√≥ƒá TYLKO czysty JSON.
FORMAT ODPOWIEDZI:
{"type": "ARTYKU≈Å" lub "REKLAMA", "formatted_text": "Sformatowany tekst w markdown."}"""
    def get_meta_tags_prompt(self) -> str:
        return """Jeste≈õ ekspertem SEO. Na podstawie poni≈ºszego tekstu artyku≈Çu, wygeneruj chwytliwy meta title i zwiƒôz≈Çy meta description.
WYMAGANIA:
- Meta title: max 60 znak√≥w.
- Meta description: max 160 znak√≥w.
- Odpowied≈∫ zwr√≥ƒá jako czysty obiekt JSON bez markdown code blocks.
FORMAT ODPOWIEDZI:
{"meta_title": "Tytu≈Ç meta", "meta_description": "Opis meta."}"""
    async def process_text(self, text: str, system_prompt: str, max_tokens: int = 4096) -> Dict:
        last_error = None; content = ""
        for attempt in range(MAX_RETRIES):
            try:
                response = await self.client.chat.completions.create(model=self.model, messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": text}], max_tokens=max_tokens, temperature=0.1, response_format={"type": "json_object"})
                content = response.choices[0].message.content
                if not content: raise ValueError("API zwr√≥ci≈Ço pustƒÖ odpowied≈∫.")
                return json.loads(content)
            except json.JSONDecodeError as e:
                last_error = e
                if attempt < MAX_RETRIES - 1: await asyncio.sleep(1)
                continue
            except Exception as e:
                last_error = e
                if attempt < MAX_RETRIES - 1: await asyncio.sleep(1)
                continue
        return {"error": f"B≈ÇƒÖd po {MAX_RETRIES} pr√≥bach.", "last_known_error": str(last_error), "raw_response": content}
    async def process_page(self, page_content: PageContent) -> Dict:
        page_data = {"page_number": page_content.page_number, "extraction_method": page_content.extraction_method, "ocr_confidence": page_content.ocr_confidence}
        if len(page_content.text.split()) < 20:
            page_data["type"] = "pominiƒôta"; page_data["formatted_content"] = "<i>Strona zawiera zbyt ma≈Ço tekstu.</i>"; return page_data
        result = await self.process_text(page_content.text, self.get_system_prompt(), max_tokens=4096)
        if "error" in result:
            page_data["type"] = "b≈ÇƒÖd"; page_data["formatted_content"] = f"""<div class="error-box"><strong>{result['error']}</strong><br><i>Ostatni b≈ÇƒÖd: {result['last_known_error']}</i><br><details><summary>Poka≈º surowƒÖ odpowied≈∫</summary><pre>{result['raw_response']}</pre></details></div>"""
        else:
            page_data["type"] = result.get("type", "nieznany").lower(); formatted_text = result.get("formatted_text", "")
            if page_data["type"] == "artyku≈Ç":
                page_data["formatted_content"] = markdown_to_html(formatted_text); page_data["raw_markdown"] = formatted_text
            else:
                page_data["formatted_content"] = f"<i>Zidentyfikowano jako: <strong>{page_data['type'].upper()}</strong>.</i>"
        return page_data
    async def process_article_group(self, pages_content: List[PageContent]) -> Dict:
        page_numbers = [p.page_number for p in pages_content]
        combined_text = "\n\n".join([f"--- STRONA {p.page_number} ---\n{p.text.strip()}" for p in pages_content])
        result = await self.process_text(combined_text, self.get_system_prompt(), max_tokens=8192)
        article_data = {"page_numbers": page_numbers}
        if "error" in result:
            article_data["type"] = "b≈ÇƒÖd"; article_data["formatted_content"] = f"""<div class='error-box'><strong>{result['error']}</strong><br><i>Ostatni b≈ÇƒÖd: {result['last_known_error']}</i><br><details><summary>Poka≈º surowƒÖ odpowied≈∫</summary><pre>{result['raw_response']}</pre></details></div>"""
        else:
            article_data["type"] = result.get("type", "nieznany").lower(); formatted_text = result.get("formatted_text", "")
            if article_data["type"] == "artyku≈Ç":
                article_data["formatted_content"] = markdown_to_html(formatted_text); article_data["raw_markdown"] = formatted_text
            else:
                article_data["formatted_content"] = f"<i>Zidentyfikowano jako: <strong>{article_data['type'].upper()}</strong>.</i>"
        return article_data
    def get_optimized_article_prompt(self) -> str:
        return """Jeste≈õ ekspertem content marketingu i SEO. Twoim zadaniem jest przekszta≈Çcenie zredagowanego artyku≈Çu w zoptymalizowanƒÖ wersjƒô pod publikacjƒô internetowƒÖ.
STRUKTURA ODWR√ìCONEJ PIRAMIDY:
1. Lead (1-2 akapity): Najwa≈ºniejsze informacje, odpowiedzi na pytania: kto, co, gdzie, kiedy, dlaczego
2. Rozwiniƒôcie: Szczeg√≥≈Çy, kontekst, dodatkowe informacje
3. T≈Ço: Mniej istotne szczeg√≥≈Çy, historia, dodatkowy kontekst
OPTYMALIZACJA SEO:
- Chwytliwy tytu≈Ç H1 (zawiera g≈Ç√≥wne s≈Çowo kluczowe, max 60 znak√≥w)
- ≈ör√≥dtytu≈Çy H2, H3 (zawierajƒÖ s≈Çowa kluczowe, pytania u≈ºytkownik√≥w)
- Pierwsze 100 s≈Ç√≥w zawiera g≈Ç√≥wne s≈Çowa kluczowe
- Kr√≥tkie, zrozumia≈Çe akapity (2-4 zdania)
- Pogrubienia dla wa≈ºnych termin√≥w
- Listy punktowane tam gdzie to ma sens
ZASADY:
- Zachowaj wszystkie fakty z oryginalnego tekstu
- U≈ºyj jƒôzyka naturalnego, unikaj sztuczno≈õci
- Pierwsze zdanie musi byƒá najwa≈ºniejsze i przyciƒÖgajƒÖce uwagƒô
- U≈ºywaj aktywnej strony czasownika
- Dodaj internal linking hints w [nawiasach kwadratowych]
WYMAGANIA KRYTYCZNE:
- Odpowied≈∫ TYLKO w formacie JSON
- NIE u≈ºywaj markdown code blocks (```json)
FORMAT ODPOWIEDZI:
{"optimized_title": "Chwytliwy tytu≈Ç SEO (max 60 znak√≥w)","meta_description": "Opis meta (max 160 znak√≥w)","optimized_content": "Zoptymalizowana tre≈õƒá w markdown z H1, H2, H3, **pogrubieniami**, listami","key_takeaways": ["Kluczowa informacja 1", "Kluczowa informacja 2", "Kluczowa informacja 3"],"suggested_internal_links": ["Temat 1 do linkowania", "Temat 2 do linkowania"]}"""
    async def generate_meta_tags(self, article_text: str) -> Dict:
        return await self.process_text(article_text[:4000], self.get_meta_tags_prompt(), max_tokens=200)
    async def generate_optimized_article(self, original_markdown: str) -> Dict:
        context = f"""Oto zredagowany artyku≈Ç do zoptymalizowania:\n\n---\n{original_markdown}\n---\n\nPrzekszta≈Çƒá ten artyku≈Ç zgodnie z wytycznymi."""
        return await self.process_text(context, self.get_optimized_article_prompt(), max_tokens=4096)
    def get_optimization_prompt(self) -> str:
        return """Jeste≈õ ekspertem SEO i copywriterem. Twoim zadaniem jest przekszta≈Çcenie surowego artyku≈Çu w zoptymalizowany artyku≈Ç internetowy.
ZASADY:
1. **Struktura odwr√≥conej piramidy informacji:**
   - Lead: Najwa≈ºniejsze informacje + warto≈õƒá dla czytelnika w pierwszym akapicie
   - Rozwiniƒôcie: Szczeg√≥≈Çy i kontekst w kolejnych akapitach
   - Dodatkowe informacje na ko≈Ñcu
2. **Optymalizacja SEO:**
   - Chwytliwy tytu≈Ç H1 z s≈Çowem kluczowym
   - ≈ör√≥dtytu≈Çy H2/H3 zawierajƒÖce naturalne s≈Çowa kluczowe
   - Meta description w pierwszym akapicie (150-160 znak√≥w warto≈õciowej informacji)
3. **Formatowanie:**
   - Kr√≥tkie akapity (2-4 zdania)
   - Pogrubienia dla kluczowych informacji
   - Listy punktowane gdzie ma sens
   - Podzia≈Ç na sekcje dla lepszej czytelno≈õci
4. **Styl pisania:**
   - Konkretny i warto≈õciowy
   - Aktywny tryb czasownik√≥w
   - Bezpo≈õrednie zwracanie siƒô do czytelnika (je≈õli pasuje do tematyki)
   - Eliminacja zbƒôdnych s≈Ç√≥w
WYMAGANIA KRYTYCZNE:
- Zachowaj WSZYSTKIE fakty i dane z orygina≈Çu
- Nie dodawaj informacji kt√≥re nie sƒÖ w tek≈õcie ≈∫r√≥d≈Çowym
- Odpowied≈∫ TYLKO jako czysty JSON (bez markdown blocks)
FORMAT ODPOWIEDZI:
{"optimized_title": "Chwytliwy tytu≈Ç H1","optimized_content": "Tre≈õƒá artyku≈Çu w markdown z pe≈ÇnƒÖ strukturƒÖ","key_points": ["Punkt 1", "Punkt 2", "Punkt 3"],"seo_keywords": ["s≈Çowo1", "s≈Çowo2", "s≈Çowo3"]}"""
    async def optimize_article(self, article_text: str) -> Dict:
        return await self.process_text(article_text, self.get_optimization_prompt(), max_tokens=8192)

# ===== FUNKCJE POMOCNICZE (bez zmian) =====
def markdown_to_html(text: str) -> str:
    text = text.replace('\n---\n', '\n<hr>\n')
    text = re.sub(r'^\s*# (.*?)\s*$', r'<h2>\1</h2>', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*## (.*?)\s*$', r'<h3>\1</h3>', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*### (.*?)\s*$', r'<h4>\1</h4>', text, flags=re.MULTILINE)
    text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
    paragraphs = text.split('\n\n'); html_content = []
    for para in paragraphs:
        if para.strip():
            if para.strip().startswith(('<h', '<hr')): html_content.append(para)
            else: html_content.append(f"<p>{para.strip().replace(chr(10), '<br>')}</p>")
    return ''.join(html_content)

def markdown_to_clean_html(markdown_text: str, page_number: int = None) -> str:
    html = markdown_text; html = html.replace('\n---\n', '\n<hr>\n'); html = html.replace('\n--- \n', '\n<hr>\n')
    html = re.sub(r'^\s*# (.*?)\s*$', r'<h1>\1</h1>', html, flags=re.MULTILINE)
    html = re.sub(r'^\s*## (.*?)\s*$', r'<h2>\1</h2>', html, flags=re.MULTILINE)
    html = re.sub(r'^\s*### (.*?)\s*$', r'<h3>\1</h3>', html, flags=re.MULTILINE)
    html = re.sub(r'^\s*#### (.*?)\s*$', r'<h4>\1</h4>', html, flags=re.MULTILINE)
    html = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', html)
    paragraphs = html.split('\n\n'); formatted_paragraphs = []
    for para in paragraphs:
        para = para.strip()
        if not para: continue
        if para.startswith(('<h1', '<h2', '<h3', '<h4', '<hr', '<p')): formatted_paragraphs.append(para)
        else: para_with_breaks = para.replace('\n', '<br>\n'); formatted_paragraphs.append(f'<p>{para_with_breaks}</p>')
    return '\n'.join(formatted_paragraphs)

def generate_full_html_document(content: str, title: str = "Artyku≈Ç", meta_title: str = None, meta_description: str = None) -> str:
    meta_tags = ""
    if meta_title: meta_tags += f'    <meta name="title" content="{meta_title}">\n'
    if meta_description: meta_tags += f'    <meta name="description" content="{meta_description}">\n'
    return f"""<!DOCTYPE html>\n<html lang="pl">\n<head>\n    <meta charset="UTF-8">\n    <meta name="viewport" content="width=device-width, initial-scale=1.0">\n    <title>{title}</title>\n{meta_tags}</head>\n<body>\n{content}\n</body>\n</html>"""

def get_article_html_from_page(page_index: int) -> Optional[Dict]:
    page_result = st.session_state.extracted_pages[page_index]
    if not page_result or page_result.get('type') != 'artyku≈Ç' or 'raw_markdown' not in page_result: return None
    group_pages = page_result.get('group_pages', [])
    if group_pages and len(group_pages) > 1:
        first_page_index = group_pages[0] - 1; first_page_result = st.session_state.extracted_pages[first_page_index]
        markdown_content = first_page_result.get('raw_markdown', ''); title = f"Artyku≈Ç ze stron {group_pages[0]}-{group_pages[-1]}"; pages = group_pages
    else:
        markdown_content = page_result.get('raw_markdown', ''); title = f"Artyku≈Ç ze strony {page_index + 1}"; pages = [page_index + 1]
    html_content = markdown_to_clean_html(markdown_content); meta_title = None; meta_description = None
    if page_index in st.session_state.meta_tags:
        tags = st.session_state.meta_tags[page_index]
        if 'error' not in tags: meta_title = tags.get('meta_title'); meta_description = tags.get('meta_description')
    html_document = generate_full_html_document(html_content, title=title, meta_title=meta_title, meta_description=meta_description)
    return {'html_content': html_content, 'html_document': html_document, 'title': title, 'pages': pages, 'meta_title': meta_title, 'meta_description': meta_description}

def sanitize_filename(name: str) -> str:
    if not name: return "unnamed_project"
    sanitized = re.sub(r'[\\/*?:"<>|]', "_", str(name))
    return re.sub(r'_{2,}', "_", sanitized).strip("_") or "unnamed_project"

def create_zip_archive(data: List[Dict]) -> bytes:
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        for item in data: zf.writestr(item['name'], item['content'])
    return zip_buffer.getvalue()

def parse_page_groups(input_text: str, total_pages: int) -> List[List[int]]:
    if not input_text: raise ValueError("Nie podano zakres√≥w stron.")
    groups = []; used_pages = set()
    for line in re.split(r'[;\n]+', input_text):
        line = line.strip()
        if not line: continue
        pages = []
        for part in re.split(r'[;,]+', line):
            part = part.strip()
            if not part: continue
            if '-' in part:
                start_str, end_str = part.split('-', 1)
                if not start_str.isdigit() or not end_str.isdigit(): raise ValueError(f"Niepoprawny zakres stron: '{part}'.")
                start, end = int(start_str), int(end_str)
                if start > end: raise ValueError(f"Zakres stron musi byƒá rosnƒÖcy: '{part}'.")
                if start < 1 or end > total_pages: raise ValueError(f"Zakres '{part}' wykracza poza liczbƒô stron dokumentu.")
                pages.extend(range(start, end + 1))
            else:
                if not part.isdigit(): raise ValueError(f"Niepoprawny numer strony: '{part}'.")
                page = int(part)
                if page < 1 or page > total_pages: raise ValueError(f"Strona '{page}' wykracza poza dokument.")
                pages.append(page)
        if not pages: continue
        pages = sorted(dict.fromkeys(pages))
        if any(p in used_pages for p in pages): raise ValueError(f"Strony {pages} zosta≈Çy ju≈º przypisane do innego artyku≈Çu.")
        used_pages.update(pages); groups.append(pages)
    if not groups: raise ValueError("Nie znaleziono ≈ºadnych poprawnych zakres√≥w stron.")
    return groups

# ===== ZARZƒÑDZANIE PROJEKTAMI (bez zmian) =====
def ensure_projects_dir() -> bool:
    try: PROJECTS_DIR.mkdir(exist_ok=True); return True
    except Exception as e: st.error(f"Nie mo≈ºna utworzyƒá katalogu projekt√≥w: {e}"); return False

def get_existing_projects() -> List[str]:
    if not ensure_projects_dir(): return []
    return [d.name for d in PROJECTS_DIR.iterdir() if d.is_dir()]

def save_project():
    if not st.session_state.project_name or not ensure_projects_dir(): st.error("Nie mo≈ºna zapisaƒá projektu: brak nazwy projektu."); return
    project_path = PROJECTS_DIR / st.session_state.project_name; project_path.mkdir(exist_ok=True)
    state_to_save = {k: v for k, v in st.session_state.items() if k not in ['document', 'project_loaded_and_waiting_for_file'] and not k.startswith("paddleocr_engine_")}
    state_to_save['extracted_pages'] = [p for p in st.session_state.extracted_pages if p is not None]
    try:
        with open(project_path / "project_state.json", "w", encoding="utf-8") as f: json.dump(state_to_save, f, indent=2, ensure_ascii=False)
        st.toast(f"‚úÖ Projekt '{st.session_state.project_name}' zosta≈Ç zapisany!", icon="üíæ")
    except Exception as e: st.error(f"B≈ÇƒÖd podczas zapisywania projektu: {e}")

def load_project(project_name: str):
    project_file = PROJECTS_DIR / project_name / "project_state.json"
    if not project_file.exists(): st.error(f"Plik projektu '{project_name}' nie istnieje."); return
    try:
        with open(project_file, "r", encoding="utf-8") as f: state_to_load = json.load(f)
        for key, value in state_to_load.items():
            if key != 'document': st.session_state[key] = value
        total_pages = st.session_state.get('total_pages', 0); st.session_state.extracted_pages = [None] * total_pages
        for page_data in state_to_load.get('extracted_pages', []):
            page_num_one_based = page_data.get('page_number')
            if page_num_one_based and 1 <= page_num_one_based <= total_pages: st.session_state.extracted_pages[page_num_one_based - 1] = page_data
        st.session_state.document = None; st.session_state.project_loaded_and_waiting_for_file = True
        st.success(f"‚úÖ Za≈Çadowano projekt '{project_name}'. Wgraj powiƒÖzany plik, aby kontynuowaƒá.")
    except Exception as e: st.error(f"B≈ÇƒÖd podczas ≈Çadowania projektu: {e}")

# ===== OBS≈ÅUGA PLIK√ìW =====
def handle_file_upload(uploaded_file):
    try:
        with st.spinner("≈Åadowanie pliku..."):
            file_bytes = uploaded_file.read(); document = DocumentHandler(file_bytes, uploaded_file.name)
            if st.session_state.project_loaded_and_waiting_for_file:
                if document.get_page_count() != st.session_state.total_pages: st.error(f"B≈ÇƒÖd: Wgrany plik ma {document.get_page_count()} stron, a projekt oczekuje {st.session_state.total_pages}. Wgraj w≈Ça≈õciwy plik."); return
                st.session_state.document = document; st.session_state.uploaded_filename = uploaded_file.name; st.session_state.file_type = document.file_type; st.session_state.project_loaded_and_waiting_for_file = False; st.success("‚úÖ Plik pomy≈õlnie dopasowany do projektu.")
            else:
                WIDGET_KEYS = {'api_key', 'ocr_mode', 'ocr_language', 'processing_mode', 'start_page', 'end_page', 'article_page_groups_input'}
                for key, value in SESSION_STATE_DEFAULTS.items():
                    if key not in WIDGET_KEYS: st.session_state[key] = value
                st.session_state.document = document; st.session_state.uploaded_filename = uploaded_file.name; st.session_state.file_type = document.file_type; st.session_state.project_name = sanitize_filename(Path(uploaded_file.name).stem); st.session_state.total_pages = document.get_page_count(); st.session_state.extracted_pages = [None] * document.get_page_count(); st.session_state.end_page = document.get_page_count()
                st.success(f"‚úÖ Za≈Çadowano plik: {uploaded_file.name} ({document.file_type.upper()})")
    except Exception as e: st.error(f"‚ùå B≈ÇƒÖd ≈Çadowania pliku: {e}"); st.session_state.document = None
    st.rerun()

# ===== PRZETWARZANIE AI =====
async def process_batch(ai_processor: AIProcessor, start_index: int):
    processing_limit = st.session_state.processing_end_page_index + 1; end_index = min(start_index + BATCH_SIZE, processing_limit); tasks = []
    for i in range(start_index, end_index):
        if st.session_state.document:
            page_content = st.session_state.document.get_page_content(i, force_mode=None); tasks.append(ai_processor.process_page(page_content))
    results = await asyncio.gather(*tasks, return_exceptions=True)
    for i, result in enumerate(results):
        page_index = start_index + i
        if isinstance(result, Exception): st.session_state.extracted_pages[page_index] = {"page_number": page_index + 1, "type": "b≈ÇƒÖd", "formatted_content": f"B≈ÇƒÖd: {result}"}
        else: st.session_state.extracted_pages[page_index] = result

def start_ai_processing():
    if st.session_state.processing_mode == 'article':
        try:
            groups = parse_page_groups(st.session_state.article_page_groups_input, st.session_state.total_pages)
            for group in groups:
                for page in group: st.session_state.extracted_pages[page - 1] = None
            st.session_state.article_groups = groups; st.session_state.next_article_index = 0; st.session_state.processing_status = 'in_progress'
            if groups: st.session_state.current_page = groups[0][0] - 1
        except ValueError as e: st.error(str(e)); return
    else:
        if st.session_state.processing_mode == 'all': start_idx = 0; end_idx = st.session_state.total_pages - 1
        else: start_idx = st.session_state.start_page - 1; end_idx = st.session_state.end_page - 1
        if start_idx > end_idx: st.error("Strona poczƒÖtkowa nie mo≈ºe byƒá wiƒôksza ni≈º ko≈Ñcowa."); return
        for i in range(start_idx, end_idx + 1): st.session_state.extracted_pages[i] = None
        st.session_state.processing_status = 'in_progress'; st.session_state.next_batch_start_index = start_idx; st.session_state.processing_end_page_index = end_idx; st.session_state.current_page = start_idx

def run_ai_processing_loop():
    if not st.session_state.api_key: st.error("Klucz API OpenAI nie jest skonfigurowany."); st.session_state.processing_status = 'idle'; return
    ai_processor = AIProcessor(st.session_state.api_key, st.session_state.model)
    if st.session_state.processing_mode == 'article':
        if st.session_state.next_article_index < len(st.session_state.article_groups):
            article_pages = st.session_state.article_groups[st.session_state.next_article_index]; pages_content = []
            for page_num in article_pages:
                if st.session_state.document and 0 <= page_num - 1 < st.session_state.total_pages: pages_content.append(st.session_state.document.get_page_content(page_num - 1, force_mode=None))
            article_result = asyncio.run(ai_processor.process_article_group(pages_content))
            for page in article_pages:
                page_index = page - 1
                if 0 <= page_index < len(st.session_state.extracted_pages):
                    entry = {key: value for key, value in article_result.items() if key != 'page_numbers'}; entry['page_number'] = page; entry['group_pages'] = article_pages; entry['is_group_lead'] = (page == article_pages[0]); st.session_state.extracted_pages[page_index] = entry
            st.session_state.next_article_index += 1
        else: st.session_state.processing_status = 'complete'
    else:
        if st.session_state.next_batch_start_index <= st.session_state.processing_end_page_index: asyncio.run(process_batch(ai_processor, st.session_state.next_batch_start_index)); st.session_state.next_batch_start_index += BATCH_SIZE
        else: st.session_state.processing_status = 'complete'
    st.rerun()

# ===== UI COMPONENTS =====
def init_session_state():
    if 'api_key' not in st.session_state or st.session_state.api_key is None: st.session_state.api_key = st.secrets.get("openai", {}).get("api_key")
    for key, value in SESSION_STATE_DEFAULTS.items():
        if key not in st.session_state: st.session_state[key] = value

def render_sidebar():
    with st.sidebar:
        st.header("‚öôÔ∏è Konfiguracja Projektu")
        if PADDLEOCR_AVAILABLE:
            with st.expander("üîç Silnik Ekstrakcji Tekstu (PaddleOCR)", expanded=True):
                st.info("‚ú® PaddleOCR jest w≈ÇƒÖczony jako g≈Ç√≥wny silnik!")
                st.radio("Tryb ekstrakcji:", options=['paddleocr', 'auto', 'native'], format_func=lambda x: {'paddleocr': 'üî¨ PaddleOCR (domy≈õlny - najlepsza jako≈õƒá)','auto': 'ü§ñ Auto (inteligentny wyb√≥r)','native': 'üìÑ PyMuPDF (szybki - tylko natywne PDF)'}[x], key='ocr_mode', help="""‚Ä¢ **PaddleOCR**: Zawsze u≈ºywa OCR - najlepsza jako≈õƒá, dzia≈Ça na skanach\n‚Ä¢ **Auto**: System decyduje - OCR dla skan√≥w, PyMuPDF dla natywnych\n‚Ä¢ **PyMuPDF**: Tylko dla nowoczesnych PDF z tekstem (szybkie)""")
                st.selectbox("Jƒôzyk dokumentu:", options=['pl', 'en', 'de', 'fr', 'es', 'it', 'ch_sim', 'ru'], format_func=lambda x: {'pl': 'üáµüá± Polski','en': 'üá¨üáß Angielski','de': 'üá©üá™ Niemiecki','fr': 'üá´üá∑ Francuski','es': 'üá™üá∏ Hiszpa≈Ñski','it': 'üáÆüáπ W≈Çoski','ch_sim': 'üá®üá≥ Chi≈Ñski (uproszczony)','ru': 'üá∑üá∫ Rosyjski'}[x], key='ocr_language')
                current_mode = st.session_state.get('ocr_mode', 'paddleocr')
                if current_mode == 'paddleocr': st.caption("‚ö° Tryb PaddleOCR: ~5-10s na stronƒô (CPU)")
                elif current_mode == 'auto': st.caption("‚ö° Tryb Auto: optymalna r√≥wnowaga szybko≈õƒá/jako≈õƒá")
                else: st.caption("‚ö° Tryb PyMuPDF: ~0.1s na stronƒô")
        else:
            with st.expander("‚ö†Ô∏è PaddleOCR niedostƒôpny", expanded=True): st.warning("PaddleOCR nie jest zainstalowany!"); st.code("pip install paddleocr", language="bash"); st.info("Obecnie u≈ºywany jest tylko PyMuPDF.")
        st.divider()
        projects = get_existing_projects(); selected_project = st.selectbox("Wybierz istniejƒÖcy projekt", ["Nowy projekt"] + projects)
        if st.button("Za≈Çaduj projekt", disabled=(selected_project == "Nowy projekt")): load_project(selected_project); st.rerun()
        st.divider()
        supported_formats = ["pdf"]
        if DOCX_AVAILABLE: supported_formats.append("docx")
        if MAMMOTH_AVAILABLE: supported_formats.append("doc")
        file_label = f"Wybierz plik ({', '.join(f.upper() for f in supported_formats)})"; uploaded_file = st.file_uploader(file_label, type=supported_formats)
        if uploaded_file:
            if (st.session_state.project_loaded_and_waiting_for_file or uploaded_file.name != st.session_state.get('uploaded_filename')): handle_file_upload(uploaded_file)
        if st.session_state.document:
            st.divider(); st.subheader("ü§ñ Opcje Przetwarzania")
            if st.session_state.file_type == 'pdf' and st.session_state.total_pages > 5:
                with st.expander("üí° Wskaz√≥wka: Przetwarzanie magazyn√≥w", expanded=True): st.markdown("""**Czy to skan magazynu/czasopisma?**\nüëâ U≈ºyj trybu **"Artyku≈Ç wielostronicowy"** poni≈ºej!""")
            st.radio("Wybierz tryb:", ('all', 'range', 'article'), captions=["Ca≈Çy dokument (ka≈ºda strona osobno)", "Zakres stron (ka≈ºda strona osobno)", "üì∞ Artyku≈Ç wielostronicowy (POLECANE dla magazyn√≥w!)"], key='processing_mode', horizontal=False, help="""**Artyku≈Ç wielostronicowy** - Idealny dla skan√≥w magazyn√≥w/czasopism! ≈ÅƒÖczy wybrane strony w jeden artyku≈Ç w jednym zapytaniu do AI.""")
            if st.session_state.processing_mode == 'range':
                c1, c2 = st.columns(2); c1.number_input("Od strony", min_value=1, max_value=st.session_state.total_pages, key='start_page'); c2.number_input("Do strony", min_value=st.session_state.start_page, max_value=st.session_state.total_pages, key='end_page')
            elif st.session_state.processing_mode == 'article':
                st.success("‚ú® **Tryb dla magazyn√≥w!** Podaj zakresy stron dla ka≈ºdego artyku≈Çu."); st.info("""**Przyk≈Çad dla magazynu:**\n- Artyku≈Ç 1 na str. 2-4 ‚Üí wpisz: `2-4`\n- Artyku≈Ç 2 na str. 6-8 ‚Üí wpisz: `6-8`\nKa≈ºda linia = jeden artyku≈Ç!"""); st.text_area("Zakresy stron artyku≈Ç√≥w (jeden artyku≈Ç na liniƒô)", key='article_page_groups_input', placeholder="2-4\n6-8\n10-13\n15,16,18", height=120)
            st.divider()
            processing_disabled = (st.session_state.processing_status == 'in_progress' or not st.session_state.api_key)
            button_text = ("üîÑ Przetwarzanie..." if st.session_state.processing_status == 'in_progress' else "üöÄ Rozpocznij Przetwarzanie")
            if st.button(button_text, use_container_width=True, type="primary", disabled=processing_disabled): start_ai_processing(); st.rerun()
            st.divider()
            st.info(f"**Projekt:** {st.session_state.project_name}"); st.metric("Liczba stron", st.session_state.total_pages); st.caption(f"**Format:** {st.session_state.file_type.upper()}")
            if PADDLEOCR_AVAILABLE:
                ocr_mode_label = {'paddleocr': 'üî¨ PaddleOCR', 'auto': 'ü§ñ Auto', 'native': 'üìÑ PyMuPDF'}[st.session_state.get('ocr_mode', 'paddleocr')]; st.caption(f"**Silnik:** {ocr_mode_label}")
            else: st.caption("**Silnik:** üìÑ PyMuPDF (OCR niedostƒôpny)")

def render_processing_status():
    if st.session_state.processing_status == 'idle' or not st.session_state.document: return
    processed_count = sum(1 for p in st.session_state.extracted_pages if p is not None)
    if st.session_state.processing_mode == 'article':
        total_groups = len(st.session_state.article_groups); processed_groups = st.session_state.next_article_index; progress = processed_groups / total_groups if total_groups > 0 else 0
        if st.session_state.processing_status == 'complete':
            st.success(f"‚úÖ Przetwarzanie zako≈Ñczone! Przetworzono {total_groups} artyku≈Ç(√≥w).")
            if st.session_state.article_groups and st.button("üìñ Przejd≈∫ do pierwszego artyku≈Çu", type="secondary"): st.session_state.current_page = st.session_state.article_groups[0][0] - 1; st.rerun()
        else: st.info(f"üîÑ Przetwarzanie artyku≈Ç√≥w... ({processed_groups}/{total_groups})"); st.progress(progress)
    else:
        progress = processed_count / st.session_state.total_pages if st.session_state.total_pages > 0 else 0
        if st.session_state.processing_status == 'complete':
            st.success("‚úÖ Przetwarzanie zako≈Ñczone!")
            if st.session_state.processing_mode == 'range':
                nav_button_cols = st.columns(2)
                if nav_button_cols[0].button("üìñ Przejd≈∫ do poczƒÖtku zakresu", type="secondary"): st.session_state.current_page = st.session_state.start_page - 1; st.rerun()
                if nav_button_cols[1].button("üìñ Przejd≈∫ do ko≈Ñca zakresu", type="secondary"): st.session_state.current_page = st.session_state.end_page - 1; st.rerun()
        else: st.info(f"üîÑ Przetwarzanie w toku... (Uko≈Ñczono {processed_count}/{st.session_state.total_pages} stron)"); st.progress(progress)
    c1, c2, _ = st.columns([1, 1, 3])
    if c1.button("üíæ Zapisz postƒôp", use_container_width=True): save_project()
    articles = [p for p in st.session_state.extracted_pages if p and p.get('type') == 'artyku≈Ç' and p.get('is_group_lead', True)]
    if articles:
        zip_data = [{'name': f"artykul_ze_str_{a['page_number']}.txt", 'content': a['raw_markdown'].encode('utf-8')} for a in articles if 'raw_markdown' in a]
        if zip_data: c2.download_button("üì• Pobierz artyku≈Çy", create_zip_archive(zip_data), f"{st.session_state.project_name}_artykuly.zip", "application/zip", use_container_width=True)

def render_navigation():
    if st.session_state.total_pages <= 1: return
    st.subheader("üìñ Nawigacja")
    if st.session_state.processing_mode == 'range':
        processing_range = f"{st.session_state.start_page}-{st.session_state.end_page}"; st.info(f"üéØ Przetwarzany zakres: strony {processing_range}")
        nav_cols = st.columns(3)
        if nav_cols[0].button("‚èÆÔ∏è PoczƒÖtek zakresu", use_container_width=True): st.session_state.current_page = st.session_state.start_page - 1; st.rerun()
        if nav_cols[1].button("‚è≠Ô∏è Koniec zakresu", use_container_width=True): st.session_state.current_page = st.session_state.end_page - 1; st.rerun()
        if nav_cols[2].button("üè† PoczƒÖtek dokumentu", use_container_width=True): st.session_state.current_page = 0; st.rerun()
        st.divider()
    elif st.session_state.processing_mode == 'article' and st.session_state.article_groups:
        st.info(f"üéØ Liczba artyku≈Ç√≥w: {len(st.session_state.article_groups)}")
        article_nav_cols = st.columns(min(len(st.session_state.article_groups), 5))
        for idx, group in enumerate(st.session_state.article_groups[:5]):
            label = f"Art. {idx+1}"
            if len(group) > 1: label += f" ({group[0]}-{group[-1]})"
            else: label += f" (str. {group[0]})"
            if article_nav_cols[idx % 5].button(label, use_container_width=True, key=f"nav_art_{idx}"): st.session_state.current_page = group[0] - 1; st.rerun()
        if len(st.session_state.article_groups) > 5: st.caption(f"... i jeszcze {len(st.session_state.article_groups) - 5} artyku≈Ç√≥w")
        st.divider()
    c1, c2, c3 = st.columns([1, 2, 1])
    if c1.button("‚¨ÖÔ∏è Poprzednia", use_container_width=True, disabled=(st.session_state.current_page == 0)): st.session_state.current_page -= 1; st.rerun()
    c2.metric("Strona", f"{st.session_state.current_page + 1} / {st.session_state.total_pages}")
    if c3.button("Nastƒôpna ‚û°Ô∏è", use_container_width=True, disabled=(st.session_state.current_page >= st.session_state.total_pages - 1)): st.session_state.current_page += 1; st.rerun()
    new_page = st.slider("Przejd≈∫ do strony:", 1, st.session_state.total_pages, st.session_state.current_page + 1) - 1
    if new_page != st.session_state.current_page: st.session_state.current_page = new_page; st.rerun()

def render_page_view():
    st.divider(); page_index = st.session_state.current_page; page_content = st.session_state.document.get_page_content(page_index); pdf_col, text_col = st.columns(2, gap="large")
    with pdf_col:
        st.subheader(f"üìÑ Orygina≈Ç (Strona {page_index + 1})")
        if st.session_state.file_type == 'pdf':
            image_data = st.session_state.document.render_page_as_image(page_index)
            if image_data: st.image(image_data, use_container_width=True)
            else: st.error("Nie mo≈ºna wy≈õwietliƒá podglƒÖdu strony.")
        else: st.info(f"PodglƒÖd nie jest dostƒôpny dla plik√≥w {st.session_state.file_type.upper()}.")
        if page_content.images:
            with st.expander(f"üñºÔ∏è Poka≈º/ukryj {len(page_content.images)} obraz(y)"):
                for img in page_content.images: st.image(img['image'], caption=f"Obraz {img['index'] + 1}", use_container_width=True)
            img_zip = create_zip_archive([{'name': f"str_{page_index+1}_img_{i['index']}.{i['ext']}",'content': i['image']} for i in page_content.images])
            st.download_button("Pobierz obrazy", img_zip, f"obrazy_strona_{page_index+1}.zip", "application/zip", use_container_width=True)
    with text_col:
        st.subheader("ü§ñ Tekst przetworzony przez AI")
        extraction_method = page_content.extraction_method
        method_info = {'native': 'üìÑ PyMuPDF (natywna ekstrakcja)','paddleocr': 'üî¨ PaddleOCR (OCR - najlepsza jako≈õƒá)','paddleocr_low_conf': 'üî¨ PaddleOCR (niska pewno≈õƒá)','hybrid_native': 'üîÑ Hybrid (natywny wybrany)','native_fallback': '‚ö†Ô∏è PyMuPDF (OCR failed)'}.get(extraction_method, extraction_method)
        st.caption(f"**Metoda ekstrakcji:** {method_info}")
        if extraction_method.startswith('paddleocr') and page_content.ocr_confidence > 0:
            confidence_percent = page_content.ocr_confidence * 100; confidence_color = 'green' if confidence_percent > 80 else 'orange' if confidence_percent > 60 else 'red'; st.caption(f"**Pewno≈õƒá OCR:** :{confidence_color}[{confidence_percent:.1f}%]")
        with st.expander("üëÅÔ∏è Poka≈º surowy tekst wej≈õciowy"): st.text_area("Surowy tekst", page_content.text, height=200, disabled=True, key=f"raw_text_{page_index}")
        page_result = st.session_state.extracted_pages[page_index]
        if page_result:
            page_type = page_result.get('type', 'nieznany'); color_map = {"artyku≈Ç": "green", "reklama": "orange", "pominiƒôta": "grey", "b≈ÇƒÖd": "red"}; color = color_map.get(page_type, "red")
            st.markdown(f"**Status:** <span style='color:{color}; text-transform:uppercase;'>**{page_type}**</span>", unsafe_allow_html=True)
            group_pages = page_result.get('group_pages', [])
            if group_pages and len(group_pages) > 1: st.info(f"Ten artyku≈Ç obejmuje strony: {', '.join(str(p) for p in group_pages)}.")
            st.markdown(f"<div class='page-text-wrapper'>{page_result.get('formatted_content', '')}</div>", unsafe_allow_html=True)
            action_cols = st.columns(4)
            if action_cols[0].button("üîÑ Przetw√≥rz ponownie", key=f"reroll_{page_index}", use_container_width=True): handle_page_reroll(page_index)
            allow_meta = (page_type == 'artyku≈Ç' and 'raw_markdown' in page_result and page_result.get('is_group_lead', True))
            if action_cols[1].button("‚ú® Generuj Meta", key=f"meta_{page_index}", use_container_width=True, disabled=not allow_meta): handle_meta_tag_generation(page_index, page_result['raw_markdown'])
            if action_cols[2].button("üöÄ Optymalizuj SEO", key=f"optimize_{page_index}", use_container_width=True, disabled=not allow_meta, help="Zoptymalizuj artyku≈Ç pod SEO i strukturƒô odwr√≥conej piramidy"): handle_article_optimization(page_index, page_result['raw_markdown'])
            show_html = action_cols[3].checkbox("üìÑ Poka≈º HTML", key=f"show_html_checkbox_{page_index}", disabled=not allow_meta, help="Poka≈º i pobierz czysty HTML artyku≈Çu")
            if show_html and allow_meta:
                html_data = get_article_html_from_page(page_index)
                if html_data:
                    st.divider()
                    with st.expander("üìÑ Czysty HTML artyku≈Çu", expanded=True):
                        st.caption(f"**{html_data['title']}**"); tab1, tab2 = st.tabs(["üíª Kod HTML (zawarto≈õƒá)", "üì∞ Pe≈Çny dokument HTML"])
                        with tab1: st.code(html_data['html_content'], language='html', line_numbers=True); st.download_button(label="üì• Pobierz zawarto≈õƒá HTML", data=html_data['html_content'], file_name=f"{sanitize_filename(html_data['title'])}_content.html", mime="text/html", use_container_width=True, key=f"download_content_{page_index}")
                        with tab2: st.code(html_data['html_document'], language='html', line_numbers=True); st.download_button(label="üì• Pobierz pe≈Çny dokument HTML", data=html_data['html_document'], file_name=f"{sanitize_filename(html_data['title'])}.html", mime="text/html", use_container_width=True, key=f"download_full_{page_index}")
                        if html_data['meta_title'] or html_data['meta_description']: st.info("‚ÑπÔ∏è Ten HTML zawiera wygenerowane meta tagi SEO")
            if page_index in st.session_state.meta_tags:
                tags = st.session_state.meta_tags[page_index]
                if "error" in tags: st.error(f"B≈ÇƒÖd generowania meta tag√≥w: {tags['error']}")
                else:
                    with st.expander("Wygenerowane Meta Tagi ‚ú®", expanded=True): st.text_input("Meta Title", value=tags.get("meta_title", ""), key=f"mt_{page_index}"); st.text_area("Meta Description", value=tags.get("meta_description", ""), key=f"md_{page_index}")
            if page_index in st.session_state.get('optimized_articles', {}):
                optimized = st.session_state.optimized_articles[page_index]
                if "error" in optimized: st.error(f"B≈ÇƒÖd optymalizacji artyku≈Çu: {optimized.get('error', 'Nieznany b≈ÇƒÖd')}")
                else:
                    with st.expander("üöÄ Zoptymalizowany Artyku≈Ç (SEO + Odwr√≥cona Piramida)", expanded=True):
                        st.success("‚ú® Artyku≈Ç zosta≈Ç zoptymalizowany pod publikacjƒô internetowƒÖ!")
                        if 'meta_description' in optimized: st.info(f"**Meta Description:** {optimized['meta_description']}")
                        if 'optimized_title' in optimized: st.markdown(f"### {optimized['optimized_title']}"); st.caption("‚¨ÜÔ∏è Zoptymalizowany tytu≈Ç SEO (H1)"); st.divider()
                        if 'key_takeaways' in optimized and optimized['key_takeaways']:
                            st.markdown("**üìå Kluczowe informacje:**")
                            for point in optimized['key_takeaways']: st.markdown(f"‚Ä¢ {point}")
                            st.divider()
                        if 'optimized_content' in optimized:
                            st.markdown("**üìÑ Zoptymalizowana tre≈õƒá (struktura odwr√≥conej piramidy):**"); st.markdown(optimized['optimized_content']); st.divider()
                            if 'suggested_internal_links' in optimized and optimized['suggested_internal_links']:
                                st.markdown("**üîó Sugerowane tematy do linkowania wewnƒôtrznego:**")
                                for link_topic in optimized['suggested_internal_links']: st.markdown(f"‚Ä¢ {link_topic}")
                                st.divider()
                            col1, col2 = st.columns(2)
                            optimized_html = markdown_to_clean_html(optimized['optimized_content']); optimized_doc = generate_full_html_document(optimized_html, title=optimized.get('optimized_title', 'Artyku≈Ç'), meta_title=optimized.get('optimized_title'), meta_description=optimized.get('meta_description'))
                            col1.download_button(label="üì• Pobierz HTML", data=optimized_doc, file_name=f"{sanitize_filename(optimized.get('optimized_title', 'artykul'))}_optimized.html", mime="text/html", use_container_width=True, key=f"download_optimized_html_{page_index}")
                            col2.download_button(label="üì• Pobierz Markdown", data=optimized['optimized_content'], file_name=f"{sanitize_filename(optimized.get('optimized_title', 'artykul'))}_optimized.md", mime="text/markdown", use_container_width=True, key=f"download_optimized_md_{page_index}")
        else:
            if st.session_state.processing_status == 'in_progress': st.info("‚è≥ Strona oczekuje na przetworzenie...")
            else: st.info("Uruchom przetwarzanie w panelu bocznym.")

def handle_page_reroll(page_index: int):
    with st.spinner("Przetwarzanie strony z kontekstem..."):
        prev_text = ""; next_text = ""
        if page_index > 0: prev_content = st.session_state.document.get_page_content(page_index - 1); prev_text = prev_content.text
        curr_content = st.session_state.document.get_page_content(page_index); curr_text = curr_content.text
        if page_index < st.session_state.total_pages - 1: next_content = st.session_state.document.get_page_content(page_index + 1); next_text = next_content.text
        context_text = f"KONTEKST (POPRZEDNIA STRONA):\n{prev_text}\n\n--- STRONA DOCELOWA ---\n{curr_text}\n\nKONTEKST (NASTƒòPNA STRONA):\n{next_text}"
        ai_processor = AIProcessor(st.session_state.api_key, st.session_state.model); page_content = PageContent(page_index + 1, context_text); new_result = asyncio.run(ai_processor.process_page(page_content))
        st.session_state.extracted_pages[page_index] = new_result
    st.rerun()

def handle_meta_tag_generation(page_index: int, raw_markdown: str):
    with st.spinner("Generowanie meta tag√≥w..."):
        ai_processor = AIProcessor(st.session_state.api_key, st.session_state.model); tags = asyncio.run(ai_processor.generate_meta_tags(raw_markdown)); st.session_state.meta_tags[page_index] = tags
    st.rerun()

def handle_article_optimization(page_index: int, raw_markdown: str):
    with st.spinner("üöÄ Optymalizacja artyku≈Çu... To mo≈ºe chwilƒô potrwaƒá."):
        ai_processor = AIProcessor(st.session_state.api_key, st.session_state.model); optimized = asyncio.run(ai_processor.generate_optimized_article(raw_markdown))
        if 'optimized_articles' not in st.session_state: st.session_state.optimized_articles = {}
        st.session_state.optimized_articles[page_index] = optimized
    st.rerun()

# ===== G≈Å√ìWNA APLIKACJA =====
def main():
    st.set_page_config(layout="wide", page_title="Redaktor AI - Procesor Dokument√≥w + OCR", page_icon="üöÄ")
    st.markdown("""<style>.page-text-wrapper {border: 1px solid #e0e0e0; border-radius: 8px; padding: 20px; background-color: #f9f9f9; max-height: 600px; overflow-y: auto;} .error-box {background-color: #ffebee; border-left: 4px solid #f44336; padding: 12px; border-radius: 4px; margin: 10px 0;} .stButton button {border-radius: 8px;} h2, h3, h4 {margin-top: 1em; margin-bottom: 0.5em;}</style>""", unsafe_allow_html=True)
    st.title("üöÄ Redaktor AI - Procesor Dokument√≥w (PaddleOCR)")
    init_session_state()
    if not PADDLEOCR_AVAILABLE: st.error("‚ö†Ô∏è **Krytyczny b≈ÇƒÖd: PaddleOCR nie jest zainstalowany!**"); st.warning("Ta aplikacja wymaga biblioteki PaddleOCR. Uruchom `pip install paddleocr` w swoim ≈õrodowisku."); st.info("Je≈õli wdra≈ºasz na Streamlit Cloud, upewnij siƒô, ≈ºe `paddleocr` jest w pliku `requirements.txt`.")
    if not st.session_state.api_key: st.error("‚ùå Brak klucza API OpenAI!"); st.info("Proszƒô skonfiguruj sw√≥j klucz API w pliku `secrets.toml` w Streamlit."); st.stop()
    render_sidebar()
    if not st.session_state.document:
        if not st.session_state.project_loaded_and_waiting_for_file:
            st.info("üëã Witaj! Aby rozpoczƒÖƒá, wgraj plik (PDF/DOCX/DOC) lub za≈Çaduj istniejƒÖcy projekt z panelu bocznego.")
            with st.expander("üìñ Jak korzystaƒá z aplikacji?"):
                st.markdown("""### üî¨ PaddleOCR jako G≈Ç√≥wny Silnik\nTa aplikacja u≈ºywa **PaddleOCR** jako g≈Ç√≥wnego mechanizmu ekstrakcji tekstu, co pozwala na pracƒô ze skanami i zdjƒôciami dokument√≥w.\n\n### Tryby Ekstrakcji:\n1. **üî¨ PaddleOCR (domy≈õlny)** - Zawsze u≈ºywa OCR, najlepsza jako≈õƒá.\n2. **ü§ñ Auto** - Inteligentny wyb√≥r: OCR dla skan√≥w, PyMuPDF dla natywnych PDF.\n3. **üìÑ PyMuPDF** - Tylko natywna ekstrakcja (szybkie, ale nie dzia≈Ça na skanach).\n\n### Tryby Przetwarzania:\n1. **Ca≈Çy dokument** - Przetwarza ka≈ºdƒÖ stronƒô osobno.\n2. **Zakres stron** - Przetwarza wybrany zakres.\n3. **Artyku≈Ç wielostronicowy** - ≈ÅƒÖczy strony w jeden artyku≈Ç, idealne dla magazyn√≥w.""")
        return
    render_processing_status()
    if st.session_state.processing_status == 'in_progress': run_ai_processing_loop()
    else: render_navigation(); render_page_view()

if __name__ == "__main__":
    main()
