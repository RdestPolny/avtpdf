"""
Redaktor AI - Interaktywny Procesor Dokument√≥w z PaddleOCR
===========================================================

INSTALACJA ZALE≈ªNO≈öCI:
----------------------
# Podstawowe (jak dotychczas)
pip uninstall docx
pip install streamlit PyMuPDF openai python-docx mammoth

# NOWE: PaddleOCR (opcjonalne, ale MOCNO zalecane)
pip install paddlepaddle paddleocr

# Je≈õli masz GPU (znacznie szybsze):
pip install paddlepaddle-gpu paddleocr

URUCHOMIENIE:
streamlit run redaktor_ai_enhanced.py
"""

import streamlit as st
import fitz  # PyMuPDF
from openai import AsyncOpenAI
import io
import zipfile
import json
from pathlib import Path
import re
import asyncio
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import base64

# Opcjonalne importy
try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import mammoth
    MAMMOTH_AVAILABLE = True
except ImportError:
    MAMMOTH_AVAILABLE = False

# NOWE: PaddleOCR - Import z ochronƒÖ przed reinitialization
PADDLEOCR_AVAILABLE = False
_GLOBAL_OCR_ENGINE = None  # Globalna instancja OCR

try:
    import numpy as np
    from PIL import Image
    PADDLEOCR_AVAILABLE = True
except ImportError:
    pass

# ===== KONFIGURACJA =====

PROJECTS_DIR = Path("pdf_processor_projects")
BATCH_SIZE = 10
MAX_RETRIES = 3
DEFAULT_MODEL = 'gpt-4o-mini'

# NOWE: Konfiguracja OCR
OCR_CONFIDENCE_THRESHOLD = 0.6
NATIVE_TEXT_MIN_LENGTH = 50

SESSION_STATE_DEFAULTS = {
    'processing_status': 'idle',
    'document': None,
    'current_page': 0,
    'total_pages': 0,
    'extracted_pages': [],
    'project_name': None,
    'next_batch_start_index': 0,
    'uploaded_filename': None,
    'api_key': None,
    'model': DEFAULT_MODEL,
    'meta_tags': {},
    'project_loaded_and_waiting_for_file': False,
    'processing_mode': 'all',
    'start_page': 1,
    'end_page': 1,
    'processing_end_page_index': 0,
    'article_page_groups_input': '',
    'article_groups': [],
    'next_article_index': 0,
    'file_type': None,
    'ocr_mode': 'paddleocr',
    'ocr_language': 'pl',
    'optimized_articles': {}  # NOWE: Przechowuje zoptymalizowane wersje artyku≈Ç√≥w
}

# ===== KLASY POMOCNICZE =====

@dataclass
class PageContent:
    """Reprezentuje zawarto≈õƒá pojedynczej strony"""
    page_number: int
    text: str
    images: List[Dict] = None
    extraction_method: str = "native"
    ocr_confidence: float = 0.0
    
    def __post_init__(self):
        if self.images is None:
            self.images = []

def get_or_create_ocr_engine(language: str = 'pl'):
    """
    Singleton OCR engine z globalnƒÖ instancjƒÖ
    Zapobiega b≈Çƒôdowi reinicjalizacji PDX
    """
    global _GLOBAL_OCR_ENGINE
    
    if not PADDLEOCR_AVAILABLE:
        raise RuntimeError("PaddleOCR nie jest zainstalowany!")
    
    # Je≈õli ju≈º mamy instancjƒô - zwr√≥ƒá jƒÖ
    if _GLOBAL_OCR_ENGINE is not None:
        return _GLOBAL_OCR_ENGINE
    
    # Lazy import - dopiero tutaj importujemy
    try:
        # Sprawd≈∫ czy PaddleX jest ju≈º zainicjalizowany
        import sys
        if 'paddlex' in sys.modules:
            # PaddleX ju≈º za≈Çadowany - spr√≥buj u≈ºyƒá istniejƒÖcej konfiguracji
            try:
                from paddleocr import PaddleOCR
                _GLOBAL_OCR_ENGINE = PaddleOCR(
                    use_angle_cls=True,
                    lang=language,
                    show_log=False,
                    use_gpu=False
                )
                return _GLOBAL_OCR_ENGINE
            except Exception as inner_e:
                # Je≈õli to b≈ÇƒÖd reinicjalizacji, zignoruj i spr√≥buj ponownie
                if "already been initialized" in str(inner_e):
                    # Wyczy≈õƒá modu≈Ç i spr√≥buj ponownie
                    if 'paddlex' in sys.modules:
                        del sys.modules['paddlex']
                    if 'paddleocr' in sys.modules:
                        del sys.modules['paddleocr']
                raise inner_e
        
        # Normalny import
        from paddleocr import PaddleOCR
        
        _GLOBAL_OCR_ENGINE = PaddleOCR(
            use_angle_cls=True,
            lang=language,
            show_log=False,
            use_gpu=False
        )
        return _GLOBAL_OCR_ENGINE
        
    except Exception as e:
        error_msg = str(e)
        
        # Je≈õli PDX jest ju≈º zainicjalizowany
        if "already been initialized" in error_msg:
            # Pr√≥ba workaround - zresetuj PaddleX
            try:
                import sys
                # Usu≈Ñ modu≈Çy z cache
                modules_to_remove = [k for k in sys.modules.keys() if 'paddle' in k.lower()]
                for mod in modules_to_remove:
                    del sys.modules[mod]
                
                # Spr√≥buj ponownie
                from paddleocr import PaddleOCR
                _GLOBAL_OCR_ENGINE = PaddleOCR(
                    use_angle_cls=True,
                    lang=language,
                    show_log=False,
                    use_gpu=False
                )
                return _GLOBAL_OCR_ENGINE
            except:
                pass
        
        # Ostatnia pr√≥ba - zwr√≥ƒá b≈ÇƒÖd
        st.error(f"Nie mo≈ºna zainicjalizowaƒá PaddleOCR: {error_msg}")
        st.info("üí° RozwiƒÖzanie: Zatrzymaj Streamlit (Ctrl+C) i uruchom ponownie")
        raise

def extract_text_with_paddleocr(image_data: bytes, language: str = 'pl') -> Tuple[str, float]:
    """
    WyciƒÖga tekst z obrazu u≈ºywajƒÖc PaddleOCR
    Returns: (text, average_confidence)
    """
    import numpy as np
    from PIL import Image
    
    try:
        # Pobierz globalnƒÖ instancjƒô OCR
        ocr = get_or_create_ocr_engine(language)
        
        # Konwertuj bytes na numpy array
        img = Image.open(io.BytesIO(image_data))
        img_array = np.array(img)
        
        # OCR
        result = ocr.ocr(img_array, cls=True)
        
        if not result or not result[0]:
            return "", 0.0
        
        # Parsuj wyniki
        texts = []
        confidences = []
        
        for line in result[0]:
            if line:
                text = line[1][0]
                confidence = line[1][1]
                texts.append(text)
                confidences.append(confidence)
        
        full_text = "\n".join(texts)
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        return full_text, avg_confidence
        
    except Exception as e:
        st.warning(f"B≈ÇƒÖd PaddleOCR: {e}")
        return "", 0.0

class DocumentHandler:
    """Klasa do obs≈Çugi r√≥≈ºnych format√≥w dokument√≥w - PaddleOCR jako g≈Ç√≥wny silnik"""
    
    def __init__(self, file_bytes: bytes, filename: str):
        self.file_bytes = file_bytes
        self.filename = filename
        self.file_type = self._detect_file_type(filename)
        self._document = None
        self._html_content = None
        self._load_document()
    
    def _detect_file_type(self, filename: str) -> str:
        ext = Path(filename).suffix.lower()
        if ext == '.pdf':
            return 'pdf'
        elif ext == '.docx':
            if not DOCX_AVAILABLE:
                raise ValueError("Format DOCX nie jest obs≈Çugiwany. Zainstaluj: pip install python-docx")
            return 'docx'
        elif ext == '.doc':
            if not MAMMOTH_AVAILABLE:
                raise ValueError("Format DOC nie jest obs≈Çugiwany. Zainstaluj: pip install mammoth")
            return 'doc'
        else:
            raise ValueError(f"Nieobs≈Çugiwany format pliku: {ext}")
    
    def _load_document(self):
        """≈Åaduje dokument w odpowiednim formacie"""
        if self.file_type == 'pdf':
            self._document = fitz.open(stream=self.file_bytes, filetype="pdf")
        elif self.file_type == 'docx':
            self._document = DocxDocument(io.BytesIO(self.file_bytes))
        elif self.file_type == 'doc':
            result = mammoth.convert_to_html(io.BytesIO(self.file_bytes))
            self._html_content = result.value
            self._document = None
    
    def get_page_count(self) -> int:
        """Zwraca liczbƒô stron w dokumencie"""
        if self.file_type == 'pdf':
            return len(self._document)
        elif self.file_type == 'docx':
            all_text = '\n\n'.join([p.text for p in self._document.paragraphs])
            words = all_text.split()
            return max(1, len(words) // 500 + (1 if len(words) % 500 > 0 else 0))
        elif self.file_type == 'doc':
            words = self._html_content.split()
            return max(1, len(words) // 500 + (1 if len(words) % 500 > 0 else 0))
        return 0
    
    def _should_use_ocr_primary(self, page_index: int) -> bool:
        """
        NOWA LOGIKA: PaddleOCR jako PRIMARY
        Zwraca True je≈õli PaddleOCR jest dostƒôpny i nie wy≈ÇƒÖczony przez u≈ºytkownika
        """
        if not PADDLEOCR_AVAILABLE:
            return False  # Brak PaddleOCR - u≈ºyj PyMuPDF
        
        ocr_mode = st.session_state.get('ocr_mode', 'paddleocr')  # Domy≈õlnie PaddleOCR!
        
        # Tryby:
        # - 'paddleocr': Zawsze u≈ºywaj PaddleOCR (domy≈õlny)
        # - 'auto': Inteligentny wyb√≥r
        # - 'native': Tylko PyMuPDF (fallback)
        
        if ocr_mode == 'native':
            return False  # Wymuszone PyMuPDF
        
        if ocr_mode == 'paddleocr':
            return True  # Zawsze PaddleOCR
        
        # Auto mode - sprawd≈∫ czy warto u≈ºyƒá OCR
        if self.file_type != 'pdf':
            return False  # Dla DOCX/DOC u≈ºywaj standardowej metody
        
        try:
            page = self._document.load_page(page_index)
            native_text = page.get_text("text")
            
            # Je≈õli za ma≈Ço tekstu natywnego - u≈ºyj OCR
            if len(native_text.strip()) < NATIVE_TEXT_MIN_LENGTH:
                return True
            
            # Sprawd≈∫ czy sƒÖ text blocks
            text_blocks = page.get_text("blocks")
            if not text_blocks or len(text_blocks) == 0:
                return True
            
            return False  # Jest du≈ºo natywnego tekstu - nie potrzeba OCR
            
        except:
            return True  # W razie b≈Çƒôdu - u≈ºyj OCR
    
    def get_page_content(self, page_index: int, force_mode: str = None) -> PageContent:
        """
        NOWA WERSJA: PaddleOCR jako g≈Ç√≥wny silnik
        force_mode: 'paddleocr', 'native', None (use settings)
        """
        if self.file_type != 'pdf':
            # Dla DOCX/DOC u≈ºywamy starej metody
            return self._get_non_pdf_content(page_index)
        
        # === PDF - PaddleOCR jako primary ===
        
        page = self._document.load_page(page_index)
        images = self._extract_images_from_pdf_page(page_index)
        
        # Ustal czy u≈ºywaƒá OCR
        use_ocr = force_mode == 'paddleocr' if force_mode else self._should_use_ocr_primary(page_index)
        
        if not use_ocr or not PADDLEOCR_AVAILABLE:
            # Fallback do PyMuPDF
            native_text = page.get_text("text")
            return PageContent(
                page_number=page_index + 1,
                text=native_text,
                images=images,
                extraction_method="native"
            )
        
        # === G≈Å√ìWNA ≈öCIE≈ªKA: PaddleOCR ===
        try:
            # Renderuj stronƒô w wysokiej rozdzielczo≈õci
            pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))
            img_bytes = pix.tobytes("png")
            
            # OCR
            language = st.session_state.get('ocr_language', 'pl')
            ocr_text, confidence = extract_text_with_paddleocr(img_bytes, language)
            
            # Je≈õli OCR siƒô powi√≥d≈Ç i ma wysokƒÖ jako≈õƒá
            if confidence > OCR_CONFIDENCE_THRESHOLD or len(ocr_text.strip()) > 50:
                return PageContent(
                    page_number=page_index + 1,
                    text=ocr_text,
                    images=images,
                    extraction_method="paddleocr",
                    ocr_confidence=confidence
                )
            else:
                # Niska jako≈õƒá OCR - spr√≥buj natywnego jako fallback
                native_text = page.get_text("text")
                if len(native_text.strip()) > len(ocr_text.strip()):
                    return PageContent(
                        page_number=page_index + 1,
                        text=native_text,
                        images=images,
                        extraction_method="hybrid_native",
                        ocr_confidence=confidence
                    )
                else:
                    return PageContent(
                        page_number=page_index + 1,
                        text=ocr_text,
                        images=images,
                        extraction_method="paddleocr_low_conf",
                        ocr_confidence=confidence
                    )
                
        except Exception as e:
            # OCR failed - fallback do natywnego
            st.warning(f"OCR nie powiod≈Ço siƒô dla strony {page_index + 1}: {e}")
            native_text = page.get_text("text")
            return PageContent(
                page_number=page_index + 1,
                text=native_text,
                images=images,
                extraction_method="native_fallback"
            )
    
    def _get_non_pdf_content(self, page_index: int) -> PageContent:
        """Stara metoda dla DOCX/DOC"""
        if self.file_type == 'docx':
            return self._get_docx_page_content(page_index)
        elif self.file_type == 'doc':
            return self._get_doc_page_content(page_index)
    
    def _get_docx_page_content(self, page_index: int) -> PageContent:
        """Pobiera zawarto≈õƒá z DOCX - dzieli na fragmenty"""
        all_paragraphs = self._document.paragraphs
        words_per_page = 500
        
        all_text = '\n\n'.join([p.text for p in all_paragraphs])
        words = all_text.split()
        
        start_word = page_index * words_per_page
        end_word = min(start_word + words_per_page, len(words))
        
        page_text = ' '.join(words[start_word:end_word])
        images = self._extract_images_from_docx()
        
        return PageContent(page_index + 1, page_text, images)
    
    def _get_doc_page_content(self, page_index: int) -> PageContent:
        """Pobiera zawarto≈õƒá z DOC (HTML)"""
        text = re.sub('<[^<]+?>', '', self._html_content)
        words = text.split()
        words_per_page = 500
        
        start_word = page_index * words_per_page
        end_word = min(start_word + words_per_page, len(words))
        
        page_text = ' '.join(words[start_word:end_word])
        
        return PageContent(page_index + 1, page_text, [])
    
    def _extract_images_from_pdf_page(self, page_index: int) -> List[Dict]:
        """WyciƒÖga obrazy z strony PDF"""
        images = []
        if self.file_type != 'pdf':
            return images
        
        try:
            page = self._document.load_page(page_index)
            for img_index, img in enumerate(page.get_images(full=True)):
                xref = img[0]
                base_image = self._document.extract_image(xref)
                if base_image and base_image.get("width", 0) > 100 and base_image.get("height", 0) > 100:
                    images.append({
                        'image': base_image['image'],
                        'ext': base_image['ext'],
                        'index': img_index
                    })
        except Exception as e:
            st.warning(f"Nie uda≈Ço siƒô wyekstraktowaƒá obraz√≥w ze strony {page_index + 1}: {e}")
        
        return images
    
    def _extract_images_from_docx(self) -> List[Dict]:
        """WyciƒÖga obrazy z dokumentu DOCX"""
        images = []
        try:
            for rel in self._document.part.rels.values():
                if "image" in rel.target_ref:
                    img_data = rel.target_part.blob
                    ext = rel.target_ref.split('.')[-1]
                    images.append({
                        'image': img_data,
                        'ext': ext,
                        'index': len(images)
                    })
        except Exception as e:
            st.warning(f"Nie uda≈Ço siƒô wyekstraktowaƒá obraz√≥w z DOCX: {e}")
        
        return images
    
    def render_page_as_image(self, page_index: int) -> Optional[bytes]:
        """Renderuje stronƒô jako obraz (tylko dla PDF)"""
        if self.file_type != 'pdf':
            return None
        
        try:
            page = self._document.load_page(page_index)
            pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))
            return pix.tobytes("png")
        except Exception as e:
            st.error(f"B≈ÇƒÖd podczas renderowania strony {page_index + 1}: {e}")
            return None

# ===== LOGIKA AI (bez zmian, skopiowana) =====

class AIProcessor:
    """Klasa obs≈ÇugujƒÖca komunikacjƒô z OpenAI API"""
    
    def __init__(self, api_key: str, model: str = DEFAULT_MODEL):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
    
    def get_system_prompt(self) -> str:
        """Zwraca system prompt dla przetwarzania artyku≈Ç√≥w"""
        return """Jeste≈õ precyzyjnym asystentem redakcyjnym. Twoim celem jest przekszta≈Çcenie surowego tekstu w czytelny, dobrze zorganizowany artyku≈Ç internetowy.

ZASADA NADRZƒòDNA: WIERNO≈öƒÜ TRE≈öCI, ELASTYCZNO≈öƒÜ FORMY.
- Nie zmieniaj oryginalnych sformu≈Çowa≈Ñ ani nie parafrazuj tekstu. Przenie≈õ tre≈õƒá 1:1.
- Twoja rola polega na dodawaniu element√≥w strukturalnych (nag≈Ç√≥wki, pogrubienia, podzia≈Ç na akapity).

INSTRUKCJE SPECJALNE:
1. Ignoruj i pomijaj numery stron oraz rozstrzelone daty.
2. Etykiety jak "NEWS FLASH" u≈ºywaj jako kontekstu, ale nie umieszczaj ich w finalnym tek≈õcie.
3. Je≈õli tekst zawiera b≈Çƒôdy OCR lub dziwne znaki, spr√≥buj je poprawiƒá w kontek≈õcie.

DOZWOLONE MODYFIKACJE STRUKTURALNE:
1. Tytu≈Ç G≈Ç√≥wny: `# Tytu≈Ç`
2. ≈ör√≥dtytu≈Çy: `## ≈ör√≥dtytu≈Ç` (u≈ºywaj ich do rozbijania '≈õciany tekstu').
3. Pogrubienia: `**tekst**` (dla kluczowych termin√≥w i nazw w≈Çasnych).
4. Podzia≈Ç na sekcje: `---` (je≈õli na stronie sƒÖ dwa niepowiƒÖzane tematy).

WYMAGANIA KRYTYCZNE:
- Twoja odpowied≈∫ musi byƒá WY≈ÅƒÑCZNIE i BEZWZGLƒòDNIE poprawnym obiektem JSON.
- NIE u≈ºywaj markdown code blocks (```json). Zwr√≥ƒá TYLKO czysty JSON.

FORMAT ODPOWIEDZI:
{"type": "ARTYKU≈Å" lub "REKLAMA", "formatted_text": "Sformatowany tekst w markdown."}"""
    
    def get_meta_tags_prompt(self) -> str:
        """Zwraca prompt dla generowania meta tag√≥w"""
        return """Jeste≈õ ekspertem SEO. Na podstawie poni≈ºszego tekstu artyku≈Çu, wygeneruj chwytliwy meta title i zwiƒôz≈Çy meta description.

WYMAGANIA:
- Meta title: max 60 znak√≥w.
- Meta description: max 160 znak√≥w.
- Odpowied≈∫ zwr√≥ƒá jako czysty obiekt JSON bez markdown code blocks.

FORMAT ODPOWIEDZI:
{"meta_title": "Tytu≈Ç meta", "meta_description": "Opis meta."}"""
    
    async def process_text(self, text: str, system_prompt: str, max_tokens: int = 4096) -> Dict:
        """Przetwarza tekst przez OpenAI API"""
        last_error = None
        content = ""
        
        for attempt in range(MAX_RETRIES):
            try:
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": text}
                    ],
                    max_tokens=max_tokens,
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
                
                content = response.choices[0].message.content
                
                if not content:
                    raise ValueError("API zwr√≥ci≈Ço pustƒÖ odpowied≈∫.")
                
                return json.loads(content)
                
            except json.JSONDecodeError as e:
                last_error = e
                if attempt < MAX_RETRIES - 1:
                    await asyncio.sleep(1)
                continue
            except Exception as e:
                last_error = e
                if attempt < MAX_RETRIES - 1:
                    await asyncio.sleep(1)
                continue
        
        return {
            "error": f"B≈ÇƒÖd po {MAX_RETRIES} pr√≥bach.",
            "last_known_error": str(last_error),
            "raw_response": content
        }
    
    async def process_page(self, page_content: PageContent) -> Dict:
        """Przetwarza pojedynczƒÖ stronƒô"""
        page_data = {
            "page_number": page_content.page_number,
            "extraction_method": page_content.extraction_method,  # NOWE
            "ocr_confidence": page_content.ocr_confidence  # NOWE
        }
        
        if len(page_content.text.split()) < 20:
            page_data["type"] = "pominiƒôta"
            page_data["formatted_content"] = "<i>Strona zawiera zbyt ma≈Ço tekstu.</i>"
            return page_data
        
        result = await self.process_text(page_content.text, self.get_system_prompt(), max_tokens=4096)
        
        if "error" in result:
            page_data["type"] = "b≈ÇƒÖd"
            page_data["formatted_content"] = f"""<div class="error-box">
                <strong>{result['error']}</strong><br>
                <i>Ostatni b≈ÇƒÖd: {result['last_known_error']}</i><br>
                <details><summary>Poka≈º surowƒÖ odpowied≈∫</summary><pre>{result['raw_response']}</pre></details>
            </div>"""
        else:
            page_data["type"] = result.get("type", "nieznany").lower()
            formatted_text = result.get("formatted_text", "")
            
            if page_data["type"] == "artyku≈Ç":
                page_data["formatted_content"] = markdown_to_html(formatted_text)
                page_data["raw_markdown"] = formatted_text
            else:
                page_data["formatted_content"] = f"<i>Zidentyfikowano jako: <strong>{page_data['type'].upper()}</strong>.</i>"
        
        return page_data
    
    async def process_article_group(self, pages_content: List[PageContent]) -> Dict:
        """Przetwarza grupƒô stron jako jeden artyku≈Ç"""
        page_numbers = [p.page_number for p in pages_content]
        
        combined_text = "\n\n".join([
            f"--- STRONA {p.page_number} ---\n{p.text.strip()}"
            for p in pages_content
        ])
        
        result = await self.process_text(combined_text, self.get_system_prompt(), max_tokens=8192)
        
        article_data = {"page_numbers": page_numbers}
        
        if "error" in result:
            article_data["type"] = "b≈ÇƒÖd"
            article_data["formatted_content"] = f"""<div class='error-box'>
                <strong>{result['error']}</strong><br>
                <i>Ostatni b≈ÇƒÖd: {result['last_known_error']}</i><br>
                <details><summary>Poka≈º surowƒÖ odpowied≈∫</summary><pre>{result['raw_response']}</pre></details>
            </div>"""
        else:
            article_data["type"] = result.get("type", "nieznany").lower()
            formatted_text = result.get("formatted_text", "")
            
            if article_data["type"] == "artyku≈Ç":
                article_data["formatted_content"] = markdown_to_html(formatted_text)
                article_data["raw_markdown"] = formatted_text
            else:
                article_data["formatted_content"] = f"<i>Zidentyfikowano jako: <strong>{article_data['type'].upper()}</strong>.</i>"
        
        return article_data
    
    def get_optimized_article_prompt(self) -> str:
        """Zwraca prompt dla generowania zoptymalizowanego artyku≈Çu"""
        return """Jeste≈õ ekspertem content marketingu i SEO. Twoim zadaniem jest przekszta≈Çcenie zredagowanego artyku≈Çu w zoptymalizowanƒÖ wersjƒô pod publikacjƒô internetowƒÖ.

STRUKTURA ODWR√ìCONEJ PIRAMIDY:
1. Lead (1-2 akapity): Najwa≈ºniejsze informacje, odpowiedzi na pytania: kto, co, gdzie, kiedy, dlaczego
2. Rozwiniƒôcie: Szczeg√≥≈Çy, kontekst, dodatkowe informacje
3. T≈Ço: Mniej istotne szczeg√≥≈Çy, historia, dodatkowy kontekst

OPTYMALIZACJA SEO:
- Chwytliwy tytu≈Ç H1 (zawiera g≈Ç√≥wne s≈Çowo kluczowe, max 60 znak√≥w)
- ≈ör√≥dtytu≈Çy H2, H3 (zawierajƒÖ s≈Çowa kluczowe, pytania u≈ºytkownik√≥w)
- Pierwsze 100 s≈Ç√≥w zawiera g≈Ç√≥wne s≈Çowa kluczowe
- Kr√≥tkie, zrozumia≈Çe akapity (2-4 zdania)
- Pogrubienia dla wa≈ºnych termin√≥w
- Listy punktowane tam gdzie to ma sens

ZASADY:
- Zachowaj wszystkie fakty z oryginalnego tekstu
- U≈ºyj jƒôzyka naturalnego, unikaj sztuczno≈õci
- Pierwsze zdanie musi byƒá najwa≈ºniejsze i przyciƒÖgajƒÖce uwagƒô
- U≈ºywaj aktywnej strony czasownika
- Dodaj internal linking hints w [nawiasach kwadratowych]

WYMAGANIA KRYTYCZNE:
- Odpowied≈∫ TYLKO w formacie JSON
- NIE u≈ºywaj markdown code blocks (```json)

FORMAT ODPOWIEDZI:
{
  "optimized_title": "Chwytliwy tytu≈Ç SEO (max 60 znak√≥w)",
  "meta_description": "Opis meta (max 160 znak√≥w)",
  "optimized_content": "Zoptymalizowana tre≈õƒá w markdown z H1, H2, H3, **pogrubieniami**, listami",
  "key_takeaways": ["Kluczowa informacja 1", "Kluczowa informacja 2", "Kluczowa informacja 3"],
  "suggested_internal_links": ["Temat 1 do linkowania", "Temat 2 do linkowania"]
}"""
    
    async def generate_meta_tags(self, article_text: str) -> Dict:
        """Generuje meta tagi dla artyku≈Çu"""
        prompt = self.get_meta_tags_prompt()
        return await self.process_text(article_text[:4000], prompt, max_tokens=200)
    
    async def generate_optimized_article(self, original_markdown: str) -> Dict:
        """Generuje zoptymalizowanƒÖ wersjƒô artyku≈Çu"""
        prompt = self.get_optimized_article_prompt()
        context = f"""Oto zredagowany artyku≈Ç do zoptymalizowania:

---
{original_markdown}
---

Przekszta≈Çƒá ten artyku≈Ç zgodnie z wytycznymi."""
        
        return await self.process_text(context, prompt, max_tokens=4096)
    
    def get_optimization_prompt(self) -> str:
        """Zwraca prompt dla optymalizacji artyku≈Çu pod SEO"""
        return """Jeste≈õ ekspertem SEO i copywriterem. Twoim zadaniem jest przekszta≈Çcenie surowego artyku≈Çu w zoptymalizowany artyku≈Ç internetowy.

ZASADY:
1. **Struktura odwr√≥conej piramidy informacji:**
   - Lead: Najwa≈ºniejsze informacje + warto≈õƒá dla czytelnika w pierwszym akapicie
   - Rozwiniƒôcie: Szczeg√≥≈Çy i kontekst w kolejnych akapitach
   - Dodatkowe informacje na ko≈Ñcu

2. **Optymalizacja SEO:**
   - Chwytliwy tytu≈Ç H1 z s≈Çowem kluczowym
   - ≈ör√≥dtytu≈Çy H2/H3 zawierajƒÖce naturalne s≈Çowa kluczowe
   - Meta description w pierwszym akapicie (150-160 znak√≥w warto≈õciowej informacji)

3. **Formatowanie:**
   - Kr√≥tkie akapity (2-4 zdania)
   - Pogrubienia dla kluczowych informacji
   - Listy punktowane gdzie ma sens
   - Podzia≈Ç na sekcje dla lepszej czytelno≈õci

4. **Styl pisania:**
   - Konkretny i warto≈õciowy
   - Aktywny tryb czasownik√≥w
   - Bezpo≈õrednie zwracanie siƒô do czytelnika (je≈õli pasuje do tematyki)
   - Eliminacja zbƒôdnych s≈Ç√≥w

WYMAGANIA KRYTYCZNE:
- Zachowaj WSZYSTKIE fakty i dane z orygina≈Çu
- Nie dodawaj informacji kt√≥re nie sƒÖ w tek≈õcie ≈∫r√≥d≈Çowym
- Odpowied≈∫ TYLKO jako czysty JSON (bez markdown blocks)

FORMAT ODPOWIEDZI:
{
  "optimized_title": "Chwytliwy tytu≈Ç H1",
  "optimized_content": "Tre≈õƒá artyku≈Çu w markdown z pe≈ÇnƒÖ strukturƒÖ",
  "key_points": ["Punkt 1", "Punkt 2", "Punkt 3"],
  "seo_keywords": ["s≈Çowo1", "s≈Çowo2", "s≈Çowo3"]
}"""
    
    async def optimize_article(self, article_text: str) -> Dict:
        """
        Optymalizuje artyku≈Ç pod SEO i strukturƒô odwr√≥conej piramidy
        """
        prompt = self.get_optimization_prompt()
        return await self.process_text(article_text, prompt, max_tokens=8192)

# ===== FUNKCJE POMOCNICZE (bez zmian) =====

def markdown_to_html(text: str) -> str:
    """Konwertuje markdown na HTML"""
    text = text.replace('\n---\n', '\n<hr>\n')
    text = re.sub(r'^\s*# (.*?)\s*$', r'<h2>\1</h2>', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*## (.*?)\s*$', r'<h3>\1</h3>', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*### (.*?)\s*$', r'<h4>\1</h4>', text, flags=re.MULTILINE)
    text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
    
    paragraphs = text.split('\n\n')
    html_content = []
    
    for para in paragraphs:
        if para.strip():
            if para.strip().startswith(('<h', '<hr')):
                html_content.append(para)
            else:
                html_content.append(f"<p>{para.strip().replace(chr(10), '<br>')}</p>")
    
    return ''.join(html_content)

def markdown_to_clean_html(markdown_text: str, page_number: int = None) -> str:
    """Konwertuje markdown na czysty HTML bez stylowania"""
    html = markdown_text
    
    html = html.replace('\n---\n', '\n<hr>\n')
    html = html.replace('\n--- \n', '\n<hr>\n')
    
    html = re.sub(r'^\s*# (.*?)\s*$', r'<h1>\1</h1>', html, flags=re.MULTILINE)
    html = re.sub(r'^\s*## (.*?)\s*$', r'<h2>\1</h2>', html, flags=re.MULTILINE)
    html = re.sub(r'^\s*### (.*?)\s*$', r'<h3>\1</h3>', html, flags=re.MULTILINE)
    html = re.sub(r'^\s*#### (.*?)\s*$', r'<h4>\1</h4>', html, flags=re.MULTILINE)
    
    html = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', html)
    
    paragraphs = html.split('\n\n')
    formatted_paragraphs = []
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        if para.startswith(('<h1', '<h2', '<h3', '<h4', '<hr', '<p')):
            formatted_paragraphs.append(para)
        else:
            para_with_breaks = para.replace('\n', '<br>\n')
            formatted_paragraphs.append(f'<p>{para_with_breaks}</p>')
    
    return '\n'.join(formatted_paragraphs)

def generate_full_html_document(content: str, title: str = "Artyku≈Ç", meta_title: str = None, meta_description: str = None) -> str:
    """Generuje pe≈Çny dokument HTML z czystƒÖ strukturƒÖ"""
    meta_tags = ""
    if meta_title:
        meta_tags += f'    <meta name="title" content="{meta_title}">\n'
    if meta_description:
        meta_tags += f'    <meta name="description" content="{meta_description}">\n'
    
    html_doc = f"""<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
{meta_tags}</head>
<body>
{content}
</body>
</html>"""
    
    return html_doc

def get_article_html_from_page(page_index: int) -> Optional[Dict]:
    """Pobiera czysty HTML artyku≈Çu dla danej strony"""
    page_result = st.session_state.extracted_pages[page_index]
    
    if not page_result or page_result.get('type') != 'artyku≈Ç':
        return None
    
    if 'raw_markdown' not in page_result:
        return None
    
    group_pages = page_result.get('group_pages', [])
    
    if group_pages and len(group_pages) > 1:
        first_page_index = group_pages[0] - 1
        first_page_result = st.session_state.extracted_pages[first_page_index]
        
        markdown_content = first_page_result.get('raw_markdown', '')
        title = f"Artyku≈Ç ze stron {group_pages[0]}-{group_pages[-1]}"
        pages = group_pages
    else:
        markdown_content = page_result.get('raw_markdown', '')
        title = f"Artyku≈Ç ze strony {page_index + 1}"
        pages = [page_index + 1]
    
    html_content = markdown_to_clean_html(markdown_content)
    
    meta_title = None
    meta_description = None
    
    if page_index in st.session_state.meta_tags:
        tags = st.session_state.meta_tags[page_index]
        if 'error' not in tags:
            meta_title = tags.get('meta_title')
            meta_description = tags.get('meta_description')
    
    html_document = generate_full_html_document(
        html_content,
        title=title,
        meta_title=meta_title,
        meta_description=meta_description
    )
    
    return {
        'html_content': html_content,
        'html_document': html_document,
        'title': title,
        'pages': pages,
        'meta_title': meta_title,
        'meta_description': meta_description
    }

def sanitize_filename(name: str) -> str:
    """Sanityzuje nazwƒô pliku"""
    if not name:
        return "unnamed_project"
    sanitized = re.sub(r'[\\/*?:"<>|]', "_", str(name))
    return re.sub(r'_{2,}', "_", sanitized).strip("_") or "unnamed_project"

def create_zip_archive(data: List[Dict]) -> bytes:
    """Tworzy archiwum ZIP"""
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        for item in data:
            zf.writestr(item['name'], item['content'])
    return zip_buffer.getvalue()

def parse_page_groups(input_text: str, total_pages: int) -> List[List[int]]:
    """Parsuje zakresy stron z tekstu wej≈õciowego"""
    if not input_text:
        raise ValueError("Nie podano zakres√≥w stron.")
    
    groups = []
    used_pages = set()
    
    for line in re.split(r'[;\n]+', input_text):
        line = line.strip()
        if not line:
            continue
        
        pages = []
        for part in re.split(r'[;,]+', line):
            part = part.strip()
            if not part:
                continue
            
            if '-' in part:
                start_str, end_str = part.split('-', 1)
                if not start_str.isdigit() or not end_str.isdigit():
                    raise ValueError(f"Niepoprawny zakres stron: '{part}'.")
                
                start, end = int(start_str), int(end_str)
                if start > end:
                    raise ValueError(f"Zakres stron musi byƒá rosnƒÖcy: '{part}'.")
                if start < 1 or end > total_pages:
                    raise ValueError(f"Zakres '{part}' wykracza poza liczbƒô stron dokumentu.")
                
                pages.extend(range(start, end + 1))
            else:
                if not part.isdigit():
                    raise ValueError(f"Niepoprawny numer strony: '{part}'.")
                
                page = int(part)
                if page < 1 or page > total_pages:
                    raise ValueError(f"Strona '{page}' wykracza poza dokument.")
                
                pages.append(page)
        
        if not pages:
            continue
        
        pages = sorted(dict.fromkeys(pages))
        
        if any(p in used_pages for p in pages):
            raise ValueError(f"Strony {pages} zosta≈Çy ju≈º przypisane do innego artyku≈Çu.")
        
        used_pages.update(pages)
        groups.append(pages)
    
    if not groups:
        raise ValueError("Nie znaleziono ≈ºadnych poprawnych zakres√≥w stron.")
    
    return groups

# ===== ZARZƒÑDZANIE PROJEKTAMI (bez zmian) =====

def ensure_projects_dir() -> bool:
    """Tworzy katalog projekt√≥w je≈õli nie istnieje"""
    try:
        PROJECTS_DIR.mkdir(exist_ok=True)
        return True
    except Exception as e:
        st.error(f"Nie mo≈ºna utworzyƒá katalogu projekt√≥w: {e}")
        return False

def get_existing_projects() -> List[str]:
    """Zwraca listƒô istniejƒÖcych projekt√≥w"""
    if not ensure_projects_dir():
        return []
    return [d.name for d in PROJECTS_DIR.iterdir() if d.is_dir()]

def save_project():
    """Zapisuje projekt do pliku"""
    if not st.session_state.project_name or not ensure_projects_dir():
        st.error("Nie mo≈ºna zapisaƒá projektu: brak nazwy projektu.")
        return
    
    project_path = PROJECTS_DIR / st.session_state.project_name
    project_path.mkdir(exist_ok=True)
    
    state_to_save = {
        k: v for k, v in st.session_state.items()
        if k not in ['document', 'project_loaded_and_waiting_for_file']
    }
    state_to_save['extracted_pages'] = [
        p for p in st.session_state.extracted_pages if p is not None
    ]
    
    try:
        with open(project_path / "project_state.json", "w", encoding="utf-8") as f:
            json.dump(state_to_save, f, indent=2, ensure_ascii=False)
        st.toast(f"‚úÖ Projekt '{st.session_state.project_name}' zosta≈Ç zapisany!", icon="üíæ")
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas zapisywania projektu: {e}")

def load_project(project_name: str):
    """≈Åaduje projekt z pliku"""
    project_file = PROJECTS_DIR / project_name / "project_state.json"
    
    if not project_file.exists():
        st.error(f"Plik projektu '{project_name}' nie istnieje.")
        return
    
    try:
        with open(project_file, "r", encoding="utf-8") as f:
            state_to_load = json.load(f)
        
        for key, value in state_to_load.items():
            if key != 'document':
                st.session_state[key] = value
        
        total_pages = st.session_state.get('total_pages', 0)
        st.session_state.extracted_pages = [None] * total_pages
        
        for page_data in state_to_load.get('extracted_pages', []):
            page_num_one_based = page_data.get('page_number')
            if page_num_one_based and 1 <= page_num_one_based <= total_pages:
                st.session_state.extracted_pages[page_num_one_based - 1] = page_data
        
        st.session_state.document = None
        st.session_state.project_loaded_and_waiting_for_file = True
        
        st.success(f"‚úÖ Za≈Çadowano projekt '{project_name}'. Wgraj powiƒÖzany plik, aby kontynuowaƒá.")
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas ≈Çadowania projektu: {e}")

# ===== OBS≈ÅUGA PLIK√ìW =====

def handle_file_upload(uploaded_file):
    """Obs≈Çuguje wgranie pliku"""
    try:
        with st.spinner("≈Åadowanie pliku..."):
            file_bytes = uploaded_file.read()
            document = DocumentHandler(file_bytes, uploaded_file.name)
            
            if st.session_state.project_loaded_and_waiting_for_file:
                if document.get_page_count() != st.session_state.total_pages:
                    st.error(
                        f"B≈ÇƒÖd: Wgrany plik ma {document.get_page_count()} stron, "
                        f"a projekt oczekuje {st.session_state.total_pages}. Wgraj w≈Ça≈õciwy plik."
                    )
                    return
                
                st.session_state.document = document
                st.session_state.uploaded_filename = uploaded_file.name
                st.session_state.file_type = document.file_type
                st.session_state.project_loaded_and_waiting_for_file = False
                st.success("‚úÖ Plik pomy≈õlnie dopasowany do projektu.")
            else:
                # NAPRAWIONE: Nie resetuj kluczy widget√≥w!
                # Lista kluczy kt√≥re sƒÖ u≈ºywane przez widgety i nie mogƒÖ byƒá resetowane
                WIDGET_KEYS = {
                    'api_key', 'ocr_mode', 'ocr_language', 'processing_mode',
                    'start_page', 'end_page', 'article_page_groups_input'
                }
                
                for key, value in SESSION_STATE_DEFAULTS.items():
                    if key not in WIDGET_KEYS:  # Pomijamy klucze widget√≥w
                        st.session_state[key] = value
                
                st.session_state.document = document
                st.session_state.uploaded_filename = uploaded_file.name
                st.session_state.file_type = document.file_type
                st.session_state.project_name = sanitize_filename(Path(uploaded_file.name).stem)
                st.session_state.total_pages = document.get_page_count()
                st.session_state.extracted_pages = [None] * document.get_page_count()
                st.session_state.end_page = document.get_page_count()
                
                st.success(f"‚úÖ Za≈Çadowano plik: {uploaded_file.name} ({document.file_type.upper()})")
    
    except Exception as e:
        st.error(f"‚ùå B≈ÇƒÖd ≈Çadowania pliku: {e}")
        st.session_state.document = None
    
    st.rerun()

# ===== PRZETWARZANIE AI =====

async def process_batch(ai_processor: AIProcessor, start_index: int):
    """Przetwarza batch stron"""
    processing_limit = st.session_state.processing_end_page_index + 1
    end_index = min(start_index + BATCH_SIZE, processing_limit)
    
    tasks = []
    for i in range(start_index, end_index):
        if st.session_state.document:
            # U≈ºyj ustawienia z UI (nie force)
            page_content = st.session_state.document.get_page_content(i, force_mode=None)
            tasks.append(ai_processor.process_page(page_content))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    for i, result in enumerate(results):
        page_index = start_index + i
        if isinstance(result, Exception):
            st.session_state.extracted_pages[page_index] = {
                "page_number": page_index + 1,
                "type": "b≈ÇƒÖd",
                "formatted_content": f"B≈ÇƒÖd: {result}"
            }
        else:
            st.session_state.extracted_pages[page_index] = result

def start_ai_processing():
    """Rozpoczyna przetwarzanie AI"""
    if st.session_state.processing_mode == 'article':
        try:
            groups = parse_page_groups(
                st.session_state.article_page_groups_input,
                st.session_state.total_pages
            )
            
            for group in groups:
                for page in group:
                    st.session_state.extracted_pages[page - 1] = None
            
            st.session_state.article_groups = groups
            st.session_state.next_article_index = 0
            st.session_state.processing_status = 'in_progress'
            
            if groups:
                st.session_state.current_page = groups[0][0] - 1
            
        except ValueError as e:
            st.error(str(e))
            return
    else:
        if st.session_state.processing_mode == 'all':
            start_idx = 0
            end_idx = st.session_state.total_pages - 1
        else:
            start_idx = st.session_state.start_page - 1
            end_idx = st.session_state.end_page - 1
        
        if start_idx > end_idx:
            st.error("Strona poczƒÖtkowa nie mo≈ºe byƒá wiƒôksza ni≈º ko≈Ñcowa.")
            return
        
        for i in range(start_idx, end_idx + 1):
            st.session_state.extracted_pages[i] = None
        
        st.session_state.processing_status = 'in_progress'
        st.session_state.next_batch_start_index = start_idx
        st.session_state.processing_end_page_index = end_idx
        
        st.session_state.current_page = start_idx

def run_ai_processing_loop():
    """G≈Ç√≥wna pƒôtla przetwarzania AI"""
    if not st.session_state.api_key:
        st.error("Klucz API OpenAI nie jest skonfigurowany.")
        st.session_state.processing_status = 'idle'
        return
    
    ai_processor = AIProcessor(st.session_state.api_key, st.session_state.model)
    
    if st.session_state.processing_mode == 'article':
        if st.session_state.next_article_index < len(st.session_state.article_groups):
            article_pages = st.session_state.article_groups[st.session_state.next_article_index]
            
            pages_content = []
            for page_num in article_pages:
                if st.session_state.document and 0 <= page_num - 1 < st.session_state.total_pages:
                    pages_content.append(
                        st.session_state.document.get_page_content(page_num - 1, force_mode=None)
                    )
            
            article_result = asyncio.run(
                ai_processor.process_article_group(pages_content)
            )
            
            for page in article_pages:
                page_index = page - 1
                if 0 <= page_index < len(st.session_state.extracted_pages):
                    entry = {
                        key: value for key, value in article_result.items()
                        if key != 'page_numbers'
                    }
                    entry['page_number'] = page
                    entry['group_pages'] = article_pages
                    entry['is_group_lead'] = (page == article_pages[0])
                    st.session_state.extracted_pages[page_index] = entry
            
            st.session_state.next_article_index += 1
        else:
            st.session_state.processing_status = 'complete'
    else:
        if st.session_state.next_batch_start_index <= st.session_state.processing_end_page_index:
            asyncio.run(
                process_batch(ai_processor, st.session_state.next_batch_start_index)
            )
            st.session_state.next_batch_start_index += BATCH_SIZE
        else:
            st.session_state.processing_status = 'complete'
    
    st.rerun()

# ===== UI COMPONENTS =====

def init_session_state():
    """Inicjalizuje stan sesji"""
    if 'api_key' not in st.session_state or st.session_state.api_key is None:
        st.session_state.api_key = st.secrets.get("openai", {}).get("api_key")
    
    for key, value in SESSION_STATE_DEFAULTS.items():
        if key not in st.session_state:
            st.session_state[key] = value

def render_sidebar():
    """Renderuje panel boczny - ROZSZERZONY"""
    with st.sidebar:
        st.header("‚öôÔ∏è Konfiguracja Projektu")
        
        # === OCR SETTINGS - PaddleOCR jako g≈Ç√≥wny silnik ===
        if PADDLEOCR_AVAILABLE:
            with st.expander("üîç Silnik Ekstrakcji Tekstu (PaddleOCR)", expanded=True):
                st.info("‚ú® PaddleOCR jest w≈ÇƒÖczony jako g≈Ç√≥wny silnik!")
                
                st.radio(
                    "Tryb ekstrakcji:",
                    options=['paddleocr', 'auto', 'native'],
                    format_func=lambda x: {
                        'paddleocr': 'üî¨ PaddleOCR (domy≈õlny - najlepsza jako≈õƒá)',
                        'auto': 'ü§ñ Auto (inteligentny wyb√≥r)',
                        'native': 'üìÑ PyMuPDF (szybki - tylko natywne PDF)'
                    }[x],
                    key='ocr_mode',
                    help="""
                    ‚Ä¢ **PaddleOCR**: Zawsze u≈ºywa OCR - najlepsza jako≈õƒá, dzia≈Ça na skanach
                    ‚Ä¢ **Auto**: System decyduje - OCR dla skan√≥w, PyMuPDF dla natywnych
                    ‚Ä¢ **PyMuPDF**: Tylko dla nowoczesnych PDF z tekstem (szybkie)
                    """
                )
                
                st.selectbox(
                    "Jƒôzyk dokumentu:",
                    options=['pl', 'en', 'de', 'fr', 'es', 'it', 'ch_sim', 'ru'],
                    format_func=lambda x: {
                        'pl': 'üáµüá± Polski',
                        'en': 'üá¨üáß Angielski',
                        'de': 'üá©üá™ Niemiecki',
                        'fr': 'üá´üá∑ Francuski',
                        'es': 'üá™üá∏ Hiszpa≈Ñski',
                        'it': 'üáÆüáπ W≈Çoski',
                        'ch_sim': 'üá®üá≥ Chi≈Ñski (uproszczony)',
                        'ru': 'üá∑üá∫ Rosyjski'
                    }[x],
                    key='ocr_language'
                )
                
                # Info o wydajno≈õci
                current_mode = st.session_state.get('ocr_mode', 'paddleocr')
                if current_mode == 'paddleocr':
                    st.caption("‚ö° Tryb PaddleOCR: ~5-10s na stronƒô (CPU)")
                elif current_mode == 'auto':
                    st.caption("‚ö° Tryb Auto: optymalna r√≥wnowaga szybko≈õƒá/jako≈õƒá")
                else:
                    st.caption("‚ö° Tryb PyMuPDF: ~0.1s na stronƒô")
        else:
            with st.expander("‚ö†Ô∏è PaddleOCR niedostƒôpny", expanded=True):
                st.warning("PaddleOCR nie jest zainstalowany!")
                st.code("pip install paddleocr", language="bash")
                st.info("Obecnie u≈ºywany jest tylko PyMuPDF (dzia≈Ça tylko dla natywnych PDF)")
        
        st.divider()
        
        # Reszta bez zmian...
        projects = get_existing_projects()
        selected_project = st.selectbox(
            "Wybierz istniejƒÖcy projekt",
            ["Nowy projekt"] + projects
        )
        
        if st.button("Za≈Çaduj projekt", disabled=(selected_project == "Nowy projekt")):
            load_project(selected_project)
            st.rerun()
        
        st.divider()
        
        supported_formats = ["pdf"]
        if DOCX_AVAILABLE:
            supported_formats.append("docx")
        if MAMMOTH_AVAILABLE:
            supported_formats.append("doc")
        
        file_label = f"Wybierz plik ({', '.join(f.upper() for f in supported_formats)})"
        
        uploaded_file = st.file_uploader(
            file_label,
            type=supported_formats
        )
        
        if uploaded_file:
            if (st.session_state.project_loaded_and_waiting_for_file or
                uploaded_file.name != st.session_state.get('uploaded_filename')):
                handle_file_upload(uploaded_file)
        
        if st.session_state.document:
            st.divider()
            st.subheader("ü§ñ Opcje Przetwarzania")
            
            # NOWY: Smart tip dla magazyn√≥w
            if st.session_state.file_type == 'pdf' and st.session_state.total_pages > 5:
                with st.expander("üí° Wskaz√≥wka: Przetwarzanie magazyn√≥w", expanded=True):
                    st.markdown("""
                    **Czy to skan magazynu/czasopisma?**
                    
                    üëâ U≈ºyj trybu **"Artyku≈Ç wielostronicowy"** poni≈ºej!
                    
                    **Przyk≈Çad:**
                    - Artyku≈Ç 1: strony 2-4 ‚Üí wpisz `2-4`
                    - Artyku≈Ç 2: strony 6-8 ‚Üí wpisz `6-8`
                    - Artyku≈Ç 3: strony 10-13 ‚Üí wpisz `10-13`
                    
                    Ka≈ºdy artyku≈Ç zostanie przetworzony jako ca≈Ço≈õƒá w jednym zapytaniu!
                    """)
            
            st.radio(
                "Wybierz tryb:",
                ('all', 'range', 'article'),
                captions=[
                    "Ca≈Çy dokument (ka≈ºda strona osobno)",
                    "Zakres stron (ka≈ºda strona osobno)",
                    "üì∞ Artyku≈Ç wielostronicowy (POLECANE dla magazyn√≥w!)"
                ],
                key='processing_mode',
                horizontal=False,
                help="""
                **Artyku≈Ç wielostronicowy** - Idealny dla skan√≥w magazyn√≥w/czasopism!
                ≈ÅƒÖczy wybrane strony w jeden artyku≈Ç w jednym zapytaniu do AI.
                Np. artyku≈Ç na stronach 2-4 zostanie przetworzony jako ca≈Ço≈õƒá.
                """
            )
            
            if st.session_state.processing_mode == 'range':
                c1, c2 = st.columns(2)
                c1.number_input(
                    "Od strony",
                    min_value=1,
                    max_value=st.session_state.total_pages,
                    key='start_page'
                )
                c2.number_input(
                    "Do strony",
                    min_value=st.session_state.start_page,
                    max_value=st.session_state.total_pages,
                    key='end_page'
                )
            
            elif st.session_state.processing_mode == 'article':
                st.success("‚ú® **Tryb dla magazyn√≥w!** Podaj zakresy stron dla ka≈ºdego artyku≈Çu.")
                st.info("""
                **Przyk≈Çad dla magazynu:**
                - Artyku≈Ç 1 na str. 2-4 ‚Üí wpisz: `2-4`
                - Artyku≈Ç 2 na str. 6-8 ‚Üí wpisz: `6-8`
                - Artyku≈Ç 3 na str. 10,11,13 ‚Üí wpisz: `10,11,13`
                
                Ka≈ºda linia = jeden artyku≈Ç!
                """)
                st.text_area(
                    "Zakresy stron artyku≈Ç√≥w (jeden artyku≈Ç na liniƒô)",
                    key='article_page_groups_input',
                    placeholder="2-4\n6-8\n10-13\n15,16,18",
                    height=120,
                    help="Ka≈ºda linia to osobny artyku≈Ç. U≈ºywaj zakres√≥w (2-4) lub pojedynczych stron oddzielonych przecinkami (10,11,13)"
                )
            
            st.divider()
            
            processing_disabled = (
                st.session_state.processing_status == 'in_progress' or
                not st.session_state.api_key
            )
            
            button_text = (
                "üîÑ Przetwarzanie..."
                if st.session_state.processing_status == 'in_progress'
                else "üöÄ Rozpocznij Przetwarzanie"
            )
            
            if st.button(
                button_text,
                use_container_width=True,
                type="primary",
                disabled=processing_disabled
            ):
                start_ai_processing()
                st.rerun()
            
            st.divider()
            
            st.info(f"**Projekt:** {st.session_state.project_name}")
            st.metric("Liczba stron", st.session_state.total_pages)
            st.caption(f"**Format:** {st.session_state.file_type.upper()}")
            
            # OCR status
            if PADDLEOCR_AVAILABLE:
                ocr_mode_label = {
                    'paddleocr': 'üî¨ PaddleOCR',
                    'auto': 'ü§ñ Auto',
                    'native': 'üìÑ PyMuPDF'
                }[st.session_state.get('ocr_mode', 'paddleocr')]
                st.caption(f"**Silnik:** {ocr_mode_label}")
            else:
                st.caption("**Silnik:** üìÑ PyMuPDF (OCR niedostƒôpny)")

def render_processing_status():
    """Renderuje status przetwarzania (bez zmian)"""
    if st.session_state.processing_status == 'idle' or not st.session_state.document:
        return
    
    processed_count = sum(1 for p in st.session_state.extracted_pages if p is not None)
    
    if st.session_state.processing_mode == 'article':
        total_groups = len(st.session_state.article_groups)
        processed_groups = st.session_state.next_article_index
        progress = processed_groups / total_groups if total_groups > 0 else 0
        
        if st.session_state.processing_status == 'complete':
            st.success(f"‚úÖ Przetwarzanie zako≈Ñczone! Przetworzono {total_groups} artyku≈Ç(√≥w).")
            
            if st.session_state.article_groups:
                if st.button("üìñ Przejd≈∫ do pierwszego artyku≈Çu", type="secondary"):
                    st.session_state.current_page = st.session_state.article_groups[0][0] - 1
                    st.rerun()
        else:
            st.info(f"üîÑ Przetwarzanie artyku≈Ç√≥w... ({processed_groups}/{total_groups})")
            st.progress(progress)
    else:
        progress = processed_count / st.session_state.total_pages if st.session_state.total_pages > 0 else 0
        
        if st.session_state.processing_status == 'complete':
            st.success("‚úÖ Przetwarzanie zako≈Ñczone!")
            
            if st.session_state.processing_mode == 'range':
                nav_button_cols = st.columns(2)
                if nav_button_cols[0].button("üìñ Przejd≈∫ do poczƒÖtku zakresu", type="secondary"):
                    st.session_state.current_page = st.session_state.start_page - 1
                    st.rerun()
                if nav_button_cols[1].button("üìñ Przejd≈∫ do ko≈Ñca zakresu", type="secondary"):
                    st.session_state.current_page = st.session_state.end_page - 1
                    st.rerun()
        else:
            st.info(f"üîÑ Przetwarzanie w toku... (Uko≈Ñczono {processed_count}/{st.session_state.total_pages} stron)")
            st.progress(progress)
    
    c1, c2, _ = st.columns([1, 1, 3])
    
    if c1.button("üíæ Zapisz postƒôp", use_container_width=True):
        save_project()
    
    articles = [
        p for p in st.session_state.extracted_pages
        if p and p.get('type') == 'artyku≈Ç' and p.get('is_group_lead', True)
    ]
    
    if articles:
        zip_data = [
            {
                'name': f"artykul_ze_str_{a['page_number']}.txt",
                'content': a['raw_markdown'].encode('utf-8')
            }
            for a in articles if 'raw_markdown' in a
        ]
        
        if zip_data:
            c2.download_button(
                "üì• Pobierz artyku≈Çy",
                create_zip_archive(zip_data),
                f"{st.session_state.project_name}_artykuly.zip",
                "application/zip",
                use_container_width=True
            )

def render_navigation():
    """Renderuje nawigacjƒô (bez zmian z poprzedniej wersji)"""
    if st.session_state.total_pages <= 1:
        return
    
    st.subheader("üìñ Nawigacja")
    
    if st.session_state.processing_mode == 'range':
        processing_range = f"{st.session_state.start_page}-{st.session_state.end_page}"
        st.info(f"üéØ Przetwarzany zakres: strony {processing_range}")
        
        nav_cols = st.columns(3)
        if nav_cols[0].button("‚èÆÔ∏è PoczƒÖtek zakresu", use_container_width=True):
            st.session_state.current_page = st.session_state.start_page - 1
            st.rerun()
        if nav_cols[1].button("‚è≠Ô∏è Koniec zakresu", use_container_width=True):
            st.session_state.current_page = st.session_state.end_page - 1
            st.rerun()
        if nav_cols[2].button("üè† PoczƒÖtek dokumentu", use_container_width=True):
            st.session_state.current_page = 0
            st.rerun()
        
        st.divider()
    
    elif st.session_state.processing_mode == 'article' and st.session_state.article_groups:
        st.info(f"üéØ Liczba artyku≈Ç√≥w: {len(st.session_state.article_groups)}")
        
        article_nav_cols = st.columns(min(len(st.session_state.article_groups), 5))
        for idx, group in enumerate(st.session_state.article_groups[:5]):
            label = f"Art. {idx+1}"
            if len(group) > 1:
                label += f" ({group[0]}-{group[-1]})"
            else:
                label += f" (str. {group[0]})"
            
            if article_nav_cols[idx % 5].button(label, use_container_width=True, key=f"nav_art_{idx}"):
                st.session_state.current_page = group[0] - 1
                st.rerun()
        
        if len(st.session_state.article_groups) > 5:
            st.caption(f"... i jeszcze {len(st.session_state.article_groups) - 5} artyku≈Ç√≥w")
        
        st.divider()
    
    c1, c2, c3 = st.columns([1, 2, 1])
    
    if c1.button(
        "‚¨ÖÔ∏è Poprzednia",
        use_container_width=True,
        disabled=(st.session_state.current_page == 0)
    ):
        st.session_state.current_page -= 1
        st.rerun()
    
    c2.metric("Strona", f"{st.session_state.current_page + 1} / {st.session_state.total_pages}")
    
    if c3.button(
        "Nastƒôpna ‚û°Ô∏è",
        use_container_width=True,
        disabled=(st.session_state.current_page >= st.session_state.total_pages - 1)
    ):
        st.session_state.current_page += 1
        st.rerun()
    
    new_page = st.slider(
        "Przejd≈∫ do strony:",
        1,
        st.session_state.total_pages,
        st.session_state.current_page + 1
    ) - 1
    
    if new_page != st.session_state.current_page:
        st.session_state.current_page = new_page
        st.rerun()

def render_page_view():
    """Renderuje widok strony - ROZSZERZONY"""
    st.divider()
    
    page_index = st.session_state.current_page
    page_content = st.session_state.document.get_page_content(page_index)
    
    pdf_col, text_col = st.columns(2, gap="large")
    
    with pdf_col:
        st.subheader(f"üìÑ Orygina≈Ç (Strona {page_index + 1})")
        
        if st.session_state.file_type == 'pdf':
            image_data = st.session_state.document.render_page_as_image(page_index)
            if image_data:
                st.image(image_data, use_container_width=True)
            else:
                st.error("Nie mo≈ºna wy≈õwietliƒá podglƒÖdu strony.")
        else:
            st.info(f"PodglƒÖd nie jest dostƒôpny dla plik√≥w {st.session_state.file_type.upper()}.")
        
        if page_content.images:
            with st.expander(f"üñºÔ∏è Poka≈º/ukryj {len(page_content.images)} obraz(y)"):
                for img in page_content.images:
                    st.image(
                        img['image'],
                        caption=f"Obraz {img['index'] + 1}",
                        use_container_width=True
                    )
            
            img_zip = create_zip_archive([
                {
                    'name': f"str_{page_index+1}_img_{i['index']}.{i['ext']}",
                    'content': i['image']
                }
                for i in page_content.images
            ])
            
            st.download_button(
                "Pobierz obrazy",
                img_zip,
                f"obrazy_strona_{page_index+1}.zip",
                "application/zip",
                use_container_width=True
            )
    
    with text_col:
        st.subheader("ü§ñ Tekst przetworzony przez AI")
        
        # Info o metodzie ekstrakcji
        extraction_method = page_content.extraction_method
        method_info = {
            'native': 'üìÑ PyMuPDF (natywna ekstrakcja)',
            'paddleocr': 'üî¨ PaddleOCR (OCR - najlepsza jako≈õƒá)',
            'paddleocr_low_conf': 'üî¨ PaddleOCR (niska pewno≈õƒá)',
            'hybrid_native': 'üîÑ Hybrid (natywny wybrany)',
            'native_fallback': '‚ö†Ô∏è PyMuPDF (OCR failed)'
        }.get(extraction_method, extraction_method)
        
        st.caption(f"**Metoda ekstrakcji:** {method_info}")
        
        if extraction_method.startswith('paddleocr') and page_content.ocr_confidence > 0:
            confidence_percent = page_content.ocr_confidence * 100
            confidence_color = 'green' if confidence_percent > 80 else 'orange' if confidence_percent > 60 else 'red'
            st.caption(f"**Pewno≈õƒá OCR:** :{confidence_color}[{confidence_percent:.1f}%]")
        
        with st.expander("üëÅÔ∏è Poka≈º surowy tekst wej≈õciowy"):
            st.text_area(
                "Surowy tekst",
                page_content.text,
                height=200,
                disabled=True,
                key=f"raw_text_{page_index}"
            )
        
        page_result = st.session_state.extracted_pages[page_index]
        
        if page_result:
            page_type = page_result.get('type', 'nieznany')
            color_map = {
                "artyku≈Ç": "green",
                "reklama": "orange",
                "pominiƒôta": "grey",
                "b≈ÇƒÖd": "red"
            }
            color = color_map.get(page_type, "red")
            
            st.markdown(
                f"**Status:** <span style='color:{color}; text-transform:uppercase;'>"
                f"**{page_type}**</span>",
                unsafe_allow_html=True
            )
            
            group_pages = page_result.get('group_pages', [])
            if group_pages and len(group_pages) > 1:
                st.info(f"Ten artyku≈Ç obejmuje strony: {', '.join(str(p) for p in group_pages)}.")
            
            st.markdown(
                f"<div class='page-text-wrapper'>{page_result.get('formatted_content', '')}</div>",
                unsafe_allow_html=True
            )
            
            # Przyciski akcji
            action_cols = st.columns(4)  # Zwiƒôkszone z 3 do 4
            
            if action_cols[0].button(
                "üîÑ Przetw√≥rz ponownie",
                key=f"reroll_{page_index}",
                use_container_width=True
            ):
                handle_page_reroll(page_index)
            
            allow_meta = (
                page_type == 'artyku≈Ç' and
                'raw_markdown' in page_result and
                page_result.get('is_group_lead', True)
            )
            
            if action_cols[1].button(
                "‚ú® Generuj Meta",
                key=f"meta_{page_index}",
                use_container_width=True,
                disabled=not allow_meta
            ):
                handle_meta_tag_generation(page_index, page_result['raw_markdown'])
            
            # NOWY: Przycisk optymalizacji artyku≈Çu
            if action_cols[2].button(
                "üöÄ Optymalizuj SEO",
                key=f"optimize_{page_index}",
                use_container_width=True,
                disabled=not allow_meta,
                help="Zoptymalizuj artyku≈Ç pod SEO i strukturƒô odwr√≥conej piramidy"
            ):
                handle_article_optimization(page_index, page_result['raw_markdown'])
            
            show_html = action_cols[3].checkbox(
                "üìÑ Poka≈º HTML",
                key=f"show_html_checkbox_{page_index}",
                disabled=not allow_meta,
                help="Poka≈º i pobierz czysty HTML artyku≈Çu"
            )
            
            if show_html and allow_meta:
                html_data = get_article_html_from_page(page_index)
                
                if html_data:
                    st.divider()
                    
                    with st.expander("üìÑ Czysty HTML artyku≈Çu", expanded=True):
                        st.caption(f"**{html_data['title']}**")
                        
                        tab1, tab2 = st.tabs(["üíª Kod HTML (zawarto≈õƒá)", "üì∞ Pe≈Çny dokument HTML"])
                        
                        with tab1:
                            st.code(html_data['html_content'], language='html', line_numbers=True)
                            
                            st.download_button(
                                label="üì• Pobierz zawarto≈õƒá HTML",
                                data=html_data['html_content'],
                                file_name=f"{sanitize_filename(html_data['title'])}_content.html",
                                mime="text/html",
                                use_container_width=True,
                                key=f"download_content_{page_index}"
                            )
                        
                        with tab2:
                            st.code(html_data['html_document'], language='html', line_numbers=True)
                            
                            st.download_button(
                                label="üì• Pobierz pe≈Çny dokument HTML",
                                data=html_data['html_document'],
                                file_name=f"{sanitize_filename(html_data['title'])}.html",
                                mime="text/html",
                                use_container_width=True,
                                key=f"download_full_{page_index}"
                            )
                        
                        if html_data['meta_title'] or html_data['meta_description']:
                            st.info("‚ÑπÔ∏è Ten HTML zawiera wygenerowane meta tagi SEO")
            
            if page_index in st.session_state.meta_tags:
                tags = st.session_state.meta_tags[page_index]
                
                if "error" in tags:
                    st.error(f"B≈ÇƒÖd generowania meta tag√≥w: {tags['error']}")
                else:
                    with st.expander("Wygenerowane Meta Tagi ‚ú®", expanded=True):
                        st.text_input(
                            "Meta Title",
                            value=tags.get("meta_title", ""),
                            key=f"mt_{page_index}"
                        )
                        st.text_area(
                            "Meta Description",
                            value=tags.get("meta_description", ""),
                            key=f"md_{page_index}"
                        )
            
            # NOWY: Wy≈õwietlanie zoptymalizowanego artyku≈Çu
            if page_index in st.session_state.get('optimized_articles', {}):
                optimized = st.session_state.optimized_articles[page_index]
                
                if "error" in optimized:
                    st.error(f"B≈ÇƒÖd optymalizacji artyku≈Çu: {optimized.get('error', 'Nieznany b≈ÇƒÖd')}")
                else:
                    with st.expander("üöÄ Zoptymalizowany Artyku≈Ç (SEO + Odwr√≥cona Piramida)", expanded=True):
                        st.success("‚ú® Artyku≈Ç zosta≈Ç zoptymalizowany pod publikacjƒô internetowƒÖ!")
                        
                        # Meta description
                        if 'meta_description' in optimized:
                            st.info(f"**Meta Description:** {optimized['meta_description']}")
                        
                        # Tytu≈Ç
                        if 'optimized_title' in optimized:
                            st.markdown(f"### {optimized['optimized_title']}")
                            st.caption("‚¨ÜÔ∏è Zoptymalizowany tytu≈Ç SEO (H1)")
                            st.divider()
                        
                        # Kluczowe punkty (key takeaways)
                        if 'key_takeaways' in optimized and optimized['key_takeaways']:
                            st.markdown("**üìå Kluczowe informacje:**")
                            for point in optimized['key_takeaways']:
                                st.markdown(f"‚Ä¢ {point}")
                            st.divider()
                        
                        # Zoptymalizowana tre≈õƒá
                        if 'optimized_content' in optimized:
                            st.markdown("**üìÑ Zoptymalizowana tre≈õƒá (struktura odwr√≥conej piramidy):**")
                            st.markdown(optimized['optimized_content'])
                            st.divider()
                            
                            # Sugerowane linki wewnƒôtrzne
                            if 'suggested_internal_links' in optimized and optimized['suggested_internal_links']:
                                st.markdown("**üîó Sugerowane tematy do linkowania wewnƒôtrznego:**")
                                for link_topic in optimized['suggested_internal_links']:
                                    st.markdown(f"‚Ä¢ {link_topic}")
                                st.divider()
                            
                            # Przyciski pobierania
                            col1, col2 = st.columns(2)
                            
                            # Pobierz jako HTML
                            optimized_html = markdown_to_clean_html(optimized['optimized_content'])
                            optimized_doc = generate_full_html_document(
                                optimized_html,
                                title=optimized.get('optimized_title', 'Artyku≈Ç'),
                                meta_title=optimized.get('optimized_title'),
                                meta_description=optimized.get('meta_description')
                            )
                            
                            col1.download_button(
                                label="üì• Pobierz HTML",
                                data=optimized_doc,
                                file_name=f"{sanitize_filename(optimized.get('optimized_title', 'artykul'))}_optimized.html",
                                mime="text/html",
                                use_container_width=True,
                                key=f"download_optimized_html_{page_index}"
                            )
                            
                            # Pobierz jako Markdown
                            col2.download_button(
                                label="üì• Pobierz Markdown",
                                data=optimized['optimized_content'],
                                file_name=f"{sanitize_filename(optimized.get('optimized_title', 'artykul'))}_optimized.md",
                                mime="text/markdown",
                                use_container_width=True,
                                key=f"download_optimized_md_{page_index}"
                            )
        else:
            if st.session_state.processing_status == 'in_progress':
                st.info("‚è≥ Strona oczekuje na przetworzenie...")
            else:
                st.info("Uruchom przetwarzanie w panelu bocznym.")

def handle_page_reroll(page_index: int):
    """Przetwarza stronƒô ponownie z kontekstem"""
    with st.spinner("Przetwarzanie strony z kontekstem..."):
        prev_text = ""
        if page_index > 0:
            prev_content = st.session_state.document.get_page_content(page_index - 1)
            prev_text = prev_content.text
        
        curr_content = st.session_state.document.get_page_content(page_index)
        curr_text = curr_content.text
        
        next_text = ""
        if page_index < st.session_state.total_pages - 1:
            next_content = st.session_state.document.get_page_content(page_index + 1)
            next_text = next_content.text
        
        context_text = (
            f"KONTEKST (POPRZEDNIA STRONA):\n{prev_text}\n\n"
            f"--- STRONA DOCELOWA ---\n{curr_text}\n\n"
            f"KONTEKST (NASTƒòPNA STRONA):\n{next_text}"
        )
        
        ai_processor = AIProcessor(st.session_state.api_key, st.session_state.model)
        page_content = PageContent(page_index + 1, context_text)
        new_result = asyncio.run(ai_processor.process_page(page_content))
        
        st.session_state.extracted_pages[page_index] = new_result
    
    st.rerun()

def handle_meta_tag_generation(page_index: int, raw_markdown: str):
    """Generuje meta tagi dla artyku≈Çu"""
    with st.spinner("Generowanie meta tag√≥w..."):
        ai_processor = AIProcessor(st.session_state.api_key, st.session_state.model)
        tags = asyncio.run(ai_processor.generate_meta_tags(raw_markdown))
        st.session_state.meta_tags[page_index] = tags
    
    st.rerun()

def handle_article_optimization(page_index: int, raw_markdown: str):
    """Optymalizuje artyku≈Ç pod SEO i strukturƒô odwr√≥conej piramidy"""
    with st.spinner("üöÄ Optymalizacja artyku≈Çu... To mo≈ºe chwilƒô potrwaƒá."):
        ai_processor = AIProcessor(st.session_state.api_key, st.session_state.model)
        optimized = asyncio.run(ai_processor.generate_optimized_article(raw_markdown))
        
        # Zapisz zoptymalizowanƒÖ wersjƒô
        if 'optimized_articles' not in st.session_state:
            st.session_state.optimized_articles = {}
        st.session_state.optimized_articles[page_index] = optimized
    
    st.rerun()

# ===== G≈Å√ìWNA APLIKACJA =====

def main():
    st.set_page_config(
        layout="wide",
        page_title="Redaktor AI - Procesor Dokument√≥w + OCR",
        page_icon="üöÄ"
    )
    
    st.markdown("""
    <style>
    .page-text-wrapper {
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 20px;
        background-color: #f9f9f9;
        max-height: 600px;
        overflow-y: auto;
    }
    .error-box {
        background-color: #ffebee;
        border-left: 4px solid #f44336;
        padding: 12px;
        border-radius: 4px;
        margin: 10px 0;
    }
    .stButton button {
        border-radius: 8px;
    }
    h2, h3, h4 {
        margin-top: 1em;
        margin-bottom: 0.5em;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.title("üöÄ Redaktor AI - Procesor Dokument√≥w (PaddleOCR)")
    
    init_session_state()
    
    # Warnings o brakujƒÖcych bibliotekach
    missing = []
    if not DOCX_AVAILABLE:
        missing.append("DOCX (zainstaluj: pip install python-docx)")
    if not MAMMOTH_AVAILABLE:
        missing.append("DOC (zainstaluj: pip install mammoth)")
    
    if not PADDLEOCR_AVAILABLE:
        st.error("‚ö†Ô∏è **PaddleOCR nie jest zainstalowany!**")
        st.warning("PaddleOCR jest G≈Å√ìWNYM silnikiem tej aplikacji. Bez niego dzia≈Ça tylko podstawowa ekstrakcja PyMuPDF.")
        st.code("pip install paddleocr", language="bash")
        st.info("üí° Po instalacji od≈õwie≈º stronƒô (Ctrl+R)")
    
    if missing:
        with st.sidebar:
            with st.expander("‚ö†Ô∏è Opcjonalne biblioteki", expanded=False):
                for fmt in missing:
                    st.write(f"- {fmt}")
    
    if not st.session_state.api_key:
        st.error("‚ùå Brak klucza API OpenAI!")
        st.info("Proszƒô skonfiguruj sw√≥j klucz API w Streamlit Secrets.")
        st.stop()
    
    render_sidebar()
    
    if not st.session_state.document:
        if not st.session_state.project_loaded_and_waiting_for_file:
            st.info("üëã Witaj! Aby rozpoczƒÖƒá, wgraj plik (PDF/DOCX/DOC) lub za≈Çaduj istniejƒÖcy projekt z panelu bocznego.")
            
            with st.expander("üìñ Jak korzystaƒá z aplikacji?"):
                st.markdown("""
                ### üî¨ PaddleOCR jako G≈Ç√≥wny Silnik
                
                Ta aplikacja u≈ºywa **PaddleOCR** jako g≈Ç√≥wnego mechanizmu ekstrakcji tekstu!
                
                **Zalety:**
                - ‚úÖ Dzia≈Ça na skanach i zdjƒôciach dokument√≥w
                - ‚úÖ Wykrywa tekst w obrazach
                - ‚úÖ Obs≈Çuguje 80+ jƒôzyk√≥w
                - ‚úÖ Rozpoznaje skomplikowane layouty
                
                ### Tryby Ekstrakcji:
                
                1. **üî¨ PaddleOCR (domy≈õlny)** - Zawsze u≈ºywa OCR, najlepsza jako≈õƒá
                2. **ü§ñ Auto** - Inteligentny wyb√≥r: OCR dla skan√≥w, PyMuPDF dla natywnych PDF
                3. **üìÑ PyMuPDF** - Tylko natywna ekstrakcja (szybkie, ale nie dzia≈Ça na skanach)
                
                ### Tryby Przetwarzania:
                
                1. **Ca≈Çy dokument** - Przetwarza ka≈ºdƒÖ stronƒô osobno
                2. **Zakres stron** - Przetwarza wybrany zakres
                3. **Artyku≈Ç wielostronicowy** - ≈ÅƒÖczy strony w jeden artyku≈Ç
                
                ### Obs≈Çugiwane formaty:
                - PDF (wszystkie - natywne i skany!)
                - DOCX (Microsoft Word)
                - DOC (starsze pliki Word)
                
                ### Funkcje:
                - ‚úÖ Zapisywanie i ≈Çadowanie projekt√≥w
                - ‚úÖ WyciƒÖganie grafik ze stron
                - ‚úÖ Generowanie meta tag√≥w SEO
                - ‚úÖ Eksport do HTML
                - ‚úÖ **OCR dla skan√≥w i zdjƒôƒá**
                - ‚úÖ **Obs≈Çuga wielu jƒôzyk√≥w**
                - ‚úÖ **Wykrywanie jako≈õci tekstu**
                
                ### üí° Wskaz√≥wki:
                - Dla **skan√≥w**: u≈ºyj trybu PaddleOCR (domy≈õlny)
                - Dla **nowoczesnych PDF**: mo≈ºesz u≈ºyƒá PyMuPDF (szybsze)
                - Nie wiesz?: zostaw Auto
                """)
        return
    
    render_processing_status()
    
    if st.session_state.processing_status == 'in_progress':
        run_ai_processing_loop()
    else:
        render_navigation()
        render_page_view()

if __name__ == "__main__":
    main()
